{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGT25UmKIfM1",
        "outputId": "6195ac0b-d1e2-413c-aa5e-79cb7410e83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Replace 'Your spreadsheet name' with the actual name of your spreadsheet\n",
        "worksheet = gc.open('submission_type').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_records(rows)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "HrC01wQ4w5eW",
        "outputId": "d3c16550-5c8e-4792-f2af-6812abe46d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     0  \\\n",
              "0                                                 name   \n",
              "1                            Australian Federal Police   \n",
              "2                           Professor Nicole Gillespie   \n",
              "3    Office of the Australian Information Commissio...   \n",
              "4                             Law Council of Australia   \n",
              "..                                                 ...   \n",
              "441                                         Roger Hall   \n",
              "442                                              Canva   \n",
              "443    National Emergency Communications Working Group   \n",
              "444  Australian Screen Directors Authorship Collect...   \n",
              "445                                        YawLife Pty   \n",
              "\n",
              "                                                     1           2  \n",
              "0                                        transcription       label  \n",
              "1                        Australian Federal Police.txt  Government  \n",
              "2                       Professor Nicole Gillespie.txt  Individual  \n",
              "3    Office of the Australian Information Commissio...  Government  \n",
              "4                         Law Council of Australia.txt         NGO  \n",
              "..                                                 ...         ...  \n",
              "441                                     Roger Hall.txt  Individual  \n",
              "442  Submission 505 - Canva - 18-Aug.ef9f2be867c48.pdf     Company  \n",
              "443  NECWG-ANZ_Submission_Supporting_Responsible_AI...         NGO  \n",
              "444  ADG ASDACS Submission to Safe and Responsible ...         NGO  \n",
              "445  A.I. Safety Anthony Albanese Australian Govern...     Company  \n",
              "\n",
              "[446 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4544556-effe-4c9b-b9ab-950225ff5f83\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>name</td>\n",
              "      <td>transcription</td>\n",
              "      <td>label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Australian Federal Police</td>\n",
              "      <td>Australian Federal Police.txt</td>\n",
              "      <td>Government</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Professor Nicole Gillespie</td>\n",
              "      <td>Professor Nicole Gillespie.txt</td>\n",
              "      <td>Individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Government</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Law Council of Australia</td>\n",
              "      <td>Law Council of Australia.txt</td>\n",
              "      <td>NGO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>Roger Hall</td>\n",
              "      <td>Roger Hall.txt</td>\n",
              "      <td>Individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>Canva</td>\n",
              "      <td>Submission 505 - Canva - 18-Aug.ef9f2be867c48.pdf</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>National Emergency Communications Working Group</td>\n",
              "      <td>NECWG-ANZ_Submission_Supporting_Responsible_AI...</td>\n",
              "      <td>NGO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>Australian Screen Directors Authorship Collect...</td>\n",
              "      <td>ADG ASDACS Submission to Safe and Responsible ...</td>\n",
              "      <td>NGO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>YawLife Pty</td>\n",
              "      <td>A.I. Safety Anthony Albanese Australian Govern...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>446 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4544556-effe-4c9b-b9ab-950225ff5f83')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4544556-effe-4c9b-b9ab-950225ff5f83 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4544556-effe-4c9b-b9ab-950225ff5f83');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13296330-9de9-40d2-84ad-f37a0b97e180\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13296330-9de9-40d2-84ad-f37a0b97e180')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13296330-9de9-40d2-84ad-f37a0b97e180 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d9dc08d6-9b60-4e2d-ad99-2b15fc0a8687\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d9dc08d6-9b60-4e2d-ad99-2b15fc0a8687 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 446,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 398,\n        \"samples\": [\n          \"Digital Publishers Alliance\",\n          \"Australian Screen Directors Authorship Collecting Society\",\n          \"Centre for Media Transition, UTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 398,\n        \"samples\": [\n          \"Digital Publishers Alliance.txt\",\n          \"ADG ASDACS Submission to Safe and Responsible AI Discussion in Australia final 26_7_23.78dd11a88b45.pdf\",\n          \"Centre for Media Transition, UTS.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"label\",\n          \"Government\",\n          \"University\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the header from the first row\n",
        "header = df.iloc[0]\n",
        "\n",
        "# Create a new DataFrame without the first row\n",
        "df = df[1:]\n",
        "\n",
        "# Set the extracted header as the column names\n",
        "df.columns = header\n",
        "\n",
        "# Display the modified DataFrame with the new header\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iX3AUb0E2Zsg",
        "outputId": "8edbc61c-b95a-4359-ad1a-4fcd32fea198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                               name  \\\n",
              "1                          Australian Federal Police   \n",
              "2                         Professor Nicole Gillespie   \n",
              "3  Office of the Australian Information Commissio...   \n",
              "4                           Law Council of Australia   \n",
              "5                  Digital Industry Group Inc (DIGI)   \n",
              "\n",
              "0                                      transcription       label  \n",
              "1                      Australian Federal Police.txt  Government  \n",
              "2                     Professor Nicole Gillespie.txt  Individual  \n",
              "3  Office of the Australian Information Commissio...  Government  \n",
              "4                       Law Council of Australia.txt         NGO  \n",
              "5              Digital Industry Group Inc (DIGI).txt     Company  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bc874cf-dd9f-4796-b7e9-557f0c828323\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>transcription</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Australian Federal Police</td>\n",
              "      <td>Australian Federal Police.txt</td>\n",
              "      <td>Government</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Professor Nicole Gillespie</td>\n",
              "      <td>Professor Nicole Gillespie.txt</td>\n",
              "      <td>Individual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Government</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Law Council of Australia</td>\n",
              "      <td>Law Council of Australia.txt</td>\n",
              "      <td>NGO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Digital Industry Group Inc (DIGI)</td>\n",
              "      <td>Digital Industry Group Inc (DIGI).txt</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bc874cf-dd9f-4796-b7e9-557f0c828323')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4bc874cf-dd9f-4796-b7e9-557f0c828323 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4bc874cf-dd9f-4796-b7e9-557f0c828323');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a868a02f-da0b-41e9-9bc6-34ebca4a5e44\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a868a02f-da0b-41e9-9bc6-34ebca4a5e44')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a868a02f-da0b-41e9-9bc6-34ebca4a5e44 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 445,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 397,\n        \"samples\": [\n          \"Keith Youens\",\n          \"Office of the Information Commissioner\",\n          \"Lext\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcription\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 397,\n        \"samples\": [\n          \"Keith Youens.txt\",\n          \"Office of the Information Commissioner.txt\",\n          \"Lext.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Government\",\n          \"Individual\",\n          \"Other\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create the \"Submissions-individual\" folder if it doesn't exist\n",
        "submissions_individual_folder = \"/content/drive/MyDrive/Submissions-individual\"  # Replace with your actual path\n",
        "os.makedirs(submissions_individual_folder, exist_ok=True)\n",
        "\n",
        "submissions_folder = \"/content/drive/MyDrive/Submissions\"  # Replace with your actual path\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row[\"label\"] == \"Individual\":\n",
        "        transcription_filename = row[\"transcription\"]\n",
        "\n",
        "        # Construct the source and destination file paths\n",
        "        source_file = os.path.join(submissions_folder, transcription_filename)\n",
        "        destination_file = os.path.join(submissions_individual_folder, transcription_filename)\n",
        "\n",
        "        # Check if the source file exists\n",
        "        if os.path.exists(source_file):\n",
        "            try:\n",
        "                # Move the file\n",
        "                shutil.move(source_file, destination_file)\n",
        "                print(f\"Moved file: {transcription_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving file {transcription_filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"File not found: {transcription_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ft4vV13x2iAc",
        "outputId": "7a6aa826-8f1f-45ca-835a-49a2cd3e2144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved file: Professor Nicole Gillespie.txt\n",
            "Moved file: Dr. Zofia Bednarz (University of Sydney) and Linda Przhedetsky (UTS).txt\n",
            "Moved file: Luke Thorburn, Thorin Bristow & Liam Carroll.txt\n",
            "Moved file: Professor John Swinson.txt\n",
            "Moved file: Laura Leighton.txt\n",
            "Moved file: Herbert Smith Freehills.txt\n",
            "Moved file: Ray Parker.txt\n",
            "Moved file: Anonymous.txt\n",
            "Moved file: Dr. Theresa KD Anderson, Connecting Stones Consulting and Ruth Marshall, Hocone Pty Limited.txt\n",
            "Moved file: Michael Townsend.txt\n",
            "Moved file: Rakesh Sharma (CYAINSE).txt\n",
            "Moved file: Guy Loucks.txt\n",
            "Moved file: Mitch Goodwin.txt\n",
            "Moved file: Gilbert + Tobin.txt\n",
            "Moved file: Stephan Jacobs.txt\n",
            "Moved file: Ned Cooper.txt\n",
            "Moved file: Kristen Migliorini of KomplyAi Pty Ltd..txt\n",
            "Moved file: Andrew Smailes.txt\n",
            "Moved file: Asim Das.txt\n",
            "Moved file: Dr Leigh Stephenson.txt\n",
            "Moved file: Matthew Farrugia-Roberts.txt\n",
            "Moved file: Sarah Harkness, Project Lotus.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Bradley Holland.txt\n",
            "Moved file: Hannah Maude.txt\n",
            "Moved file: Robert Chalmers.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Dr Michael Douglas.txt\n",
            "Moved file: Katherine Biewer.txt\n",
            "Moved file: Belinda Morris.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Christian McDonald.txt\n",
            "Moved file: Keith Youens.txt\n",
            "Moved file: Richard Schreurs.txt\n",
            "File not found: Henry Fraser, Christine Parker, Fiona Haines, JosÃ©-Miguel Bello y Villarino and Kimberlee Weatherall.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Eric Cameron Wilson.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: William Campos - Clinical Psychologist & Social Researcher.txt\n",
            "Moved file: Piotr Kulaga.txt\n",
            "Moved file: Paul Hadden.txt\n",
            "Moved file: Rupert McCallum.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Michael M.txt\n",
            "Moved file: Justin Olive.txt\n",
            "Moved file: Jack Larkings.txt\n",
            "Moved file: Nathan Sherburn.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Brian Bai.txt\n",
            "Moved file: Michael Kerrison.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Lucas Lewit-Mendes.txt\n",
            "Moved file: Lupo, Locke & Quoc Vo.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Chelsea Liang.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Jose-Miguel Bello y Villarino.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Dr Cameron Shackell.txt\n",
            "Moved file: David Taylor.txt\n",
            "Moved file: Rohan Fernando - STRATINNOVA.txt\n",
            "Moved file: R. Sheh and K. Geappen.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Bridget Loughhead.txt\n",
            "Moved file: Kieran Chalk.txt\n",
            "Moved file: Reason360 - Brett Feldon.txt\n",
            "Moved file: Andrew Cullen.txt\n",
            "Moved file: Dr Ramona Vijeyarasa.txt\n",
            "Moved file: Lext.txt\n",
            "Moved file: Dr Andreas Cebulla, Robert Chalmers, Dr Rajesh Johnsam, Professor Tania Leiman, Dr James Scheibner.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: James Newson.txt\n",
            "Moved file: Jenna Ong.txt\n",
            "Moved file: Tyra Burgess.txt\n",
            "Moved file: Joseph Tan.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Peter Horniak.txt\n",
            "Moved file: Sophia Cyna.txt\n",
            "Moved file: Morgan Beale.txt\n",
            "Moved file: Mark Roeder.txt\n",
            "Moved file: Dr Francina Cantatore.txt\n",
            "Moved file: Iris Vardi.txt\n",
            "Moved file: John Norman.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Martin Stewart-Weeks.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Atanaan Ilango and Benjamin Koh, Shine Lawyers.txt\n",
            "Moved file: Ben Hooper, Fingleton.txt\n",
            "Moved file: Rita Matulionyte.txt\n",
            "Moved file: Jarred Filmer.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: B How.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Natalie Morris.txt\n",
            "Moved file: Philip Derham, Derham Insights Research.txt\n",
            "Moved file: Stephen Nicholas Fowler.txt\n",
            "Moved file: James Newton-Thomas.txt\n",
            "Moved file: Dr Michael Noetel.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Fergus Dall.txt\n",
            "Moved file: Hugo Lyons Keenan.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Matt Fisher.txt\n",
            "Moved file: Ben Auer.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Manisha Lishman.txt\n",
            "Moved file: Sandstone McNamara.txt\n",
            "Moved file: Letian Wang.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Professor Dan Jerker B. Svantesson.txt\n",
            "Moved file: Stefan SK.txt\n",
            "Moved file: Ross Tieman.txt\n",
            "Moved file: Callum Dyer.txt\n",
            "Moved file: Oscar Delaney.txt\n",
            "Moved file: William Horan.txt\n",
            "Moved file: Courtney Henry.txt\n",
            "Moved file: Simon Newstead.txt\n",
            "Moved file: Scott Smith.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Jay Bailey.txt\n",
            "Moved file: Jordan Taylor.txt\n",
            "Moved file: Jack O'Brien.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Nathan Cehic.txt\n",
            "Moved file: Sam Coggins.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Jack Hawke.txt\n",
            "Moved file: Ramesh Vidyasagar.txt\n",
            "Moved file: Michael Huang.txt\n",
            "Moved file: Jo Small.txt\n",
            "Moved file: Jordan von Eitzen.txt\n",
            "Moved file: Jade Chamberlain.txt\n",
            "Moved file: Resonvate.txt\n",
            "Moved file: Kuyan H. Judith.txt\n",
            "Moved file: Evan Hockings.txt\n",
            "Moved file: Jonathan Byun.txt\n",
            "Moved file: Gregory Long.txt\n",
            "Moved file: Sami Makelainen.txt\n",
            "Moved file: Terry Aulich.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Julia Walsh.txt\n",
            "Moved file: Julia Walsh, CEO Brand Medicine International.txt\n",
            "Moved file: Jacek Korneluk & Spektrumlab Pty Ltd.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Brenda van Rensburg.txt\n",
            "Moved file: A C Stewart.txt\n",
            "Moved file: Prof. Mary Cummings.txt\n",
            "Moved file: Neil Hauxwell.txt\n",
            "Moved file: Caterina Watson.txt\n",
            "Moved file: Gabriel Furlong.txt\n",
            "Moved file: William.txt\n",
            "Moved file: Dr Simon Longstaff AO.txt\n",
            "Moved file: Linda Shave.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Ella.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Jim Brander.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Dr. Tony Carden.txt\n",
            "Moved file: Chris Drake.txt\n",
            "Moved file: Professor Rocky Scopelliti.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Andrew Robinson.txt\n",
            "Moved file: Trevor J Caldwell.txt\n",
            "Moved file: Alex Berkovich.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Lyndon Megarrity.txt\n",
            "Moved file: melissa misuraca.txt\n",
            "File not found: Anonymous.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Martin Z'Graggen.txt\n",
            "Moved file: A Palmer.txt\n",
            "Moved file: Steven.txt\n",
            "Moved file: Justin.txt\n",
            "Moved file: Matthew Tucker.txt\n",
            "Moved file: Peter Cotton.txt\n",
            "File not found: Anonymous.txt\n",
            "Moved file: Thomas Burns.txt\n",
            "Moved file: Dr Mehrdad Arashpour.txt\n",
            "Moved file: Gary Looney.txt\n",
            "Moved file: Ben Blackburn Racing.txt\n",
            "Moved file: Roger Hall.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def get_files_from_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Gets all file names and contents from a specified folder and returns them as a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        folder_path: The path to the folder.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame with 'filename' and 'content' columns, or None if the folder doesn't exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    file_data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:  # Handle potential encoding issues\n",
        "                    content = file.read()\n",
        "                file_data.append({'filename': filename, 'content': content})\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file '{filename}': {e}\")\n",
        "    return pd.DataFrame(file_data)\n",
        "\n",
        "# Example usage (replace with your actual folder path)\n",
        "submissions_folder = \"/content/drive/MyDrive/Submissions\"\n",
        "df_files = get_files_from_folder(submissions_folder)\n",
        "\n",
        "if df_files is not None:\n",
        "  df_files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw-UMPEx9E7I",
        "outputId": "3a38eba4-96a9-4c7d-8171-22b148ee9b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading file 'Submission 500 - Commonwealth Bank - 15-Aug.36c2cc1cb32ac.pdf': 'utf-8' codec can't decode byte 0xb5 in position 11: invalid start byte\n",
            "Error reading file 'Submission 505 - Canva - 18-Aug.ef9f2be867c48.pdf': 'utf-8' codec can't decode byte 0xf6 in position 10: invalid start byte\n",
            "Error reading file 'NECWG-ANZ_Submission_Supporting_Responsible_AI-July_2023_v2.ae7a663cee5cd.pdf': 'utf-8' codec can't decode byte 0xb5 in position 11: invalid start byte\n",
            "Error reading file 'ADG ASDACS Submission to Safe and Responsible AI Discussion in Australia final 26_7_23.78dd11a88b45.pdf': 'utf-8' codec can't decode byte 0xc4 in position 10: invalid continuation byte\n",
            "Error reading file 'Second Submission.6138728859d07.docx': 'utf-8' codec can't decode byte 0xf4 in position 16: invalid continuation byte\n",
            "Error reading file 'A.I. Safety Anthony Albanese Australian Government Brief.24205b04d84b5.pdf': 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
            "Error reading file 'Submission 505 - Canva - 18-Aug.ef9f2be867c48.gdoc': [Errno 95] Operation not supported: '/content/drive/MyDrive/Submissions/Submission 505 - Canva - 18-Aug.ef9f2be867c48.gdoc'\n",
            "Error reading file 'Commonwealth Bank of Australia.gdoc': [Errno 95] Operation not supported: '/content/drive/MyDrive/Submissions/Commonwealth Bank of Australia.gdoc'\n",
            "Error reading file 'Submission 500 - Commonwealth Bank - 15-Aug.36c2cc1cb32ac.gdoc': [Errno 95] Operation not supported: '/content/drive/MyDrive/Submissions/Submission 500 - Commonwealth Bank - 15-Aug.36c2cc1cb32ac.gdoc'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LAGRQne8H00Z",
        "outputId": "7580ba2c-208e-4235-e348-e04dfbbcdcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              filename  \\\n",
              "0                        Australian Federal Police.txt   \n",
              "1    Office of the Australian Information Commissio...   \n",
              "2                         Law Council of Australia.txt   \n",
              "3                Digital Industry Group Inc (DIGI).txt   \n",
              "4               Department of Health and Aged Care.txt   \n",
              "..                                                 ...   \n",
              "242                                            OOP.txt   \n",
              "243                                     Complexico.txt   \n",
              "244                                Lilin Australia.txt   \n",
              "245                                      IdeaSpies.txt   \n",
              "246                           Codemaster Institute.txt   \n",
              "\n",
              "                                               content  \n",
              "0    21 August 2023\\nAustralian Federal\\nPolice sub...  \n",
              "1    Department of Industry, Science and Resources\\...  \n",
              "2    Safe and responsible AI in\\nAustralia\\nDepartm...  \n",
              "3    Department of Industry, Science and Resources,...  \n",
              "4    Department of Health and Aged Care\\nSafe and R...  \n",
              "..                                                 ...  \n",
              "242                                sine-conic fractals  \n",
              "243  Supporting responsible AI: Complexico's submis...  \n",
              "244  No! The definitions do not incorporate devices...  \n",
              "245  IdeaSpies is an open innovation platform shari...  \n",
              "246                                                yes  \n",
              "\n",
              "[247 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a474d302-4070-4ad5-b8c4-2f76692b31d9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Australian Federal Police.txt</td>\n",
              "      <td>21 August 2023\\nAustralian Federal\\nPolice sub...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Department of Industry, Science and Resources\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Law Council of Australia.txt</td>\n",
              "      <td>Safe and responsible AI in\\nAustralia\\nDepartm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Digital Industry Group Inc (DIGI).txt</td>\n",
              "      <td>Department of Industry, Science and Resources,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Department of Health and Aged Care.txt</td>\n",
              "      <td>Department of Health and Aged Care\\nSafe and R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>OOP.txt</td>\n",
              "      <td>sine-conic fractals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>Complexico.txt</td>\n",
              "      <td>Supporting responsible AI: Complexico's submis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>Lilin Australia.txt</td>\n",
              "      <td>No! The definitions do not incorporate devices...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>IdeaSpies.txt</td>\n",
              "      <td>IdeaSpies is an open innovation platform shari...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>Codemaster Institute.txt</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>247 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a474d302-4070-4ad5-b8c4-2f76692b31d9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a474d302-4070-4ad5-b8c4-2f76692b31d9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a474d302-4070-4ad5-b8c4-2f76692b31d9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23a64375-2ae8-4b14-bc2e-2fb7a7609ce8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23a64375-2ae8-4b14-bc2e-2fb7a7609ce8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23a64375-2ae8-4b14-bc2e-2fb7a7609ce8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_47e4565d-3979-4fae-a2af-7f1c7c24cc99\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_files')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_47e4565d-3979-4fae-a2af-7f1c7c24cc99 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_files');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_files",
              "summary": "{\n  \"name\": \"df_files\",\n  \"rows\": 247,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 247,\n        \"samples\": [\n          \"Digital Media Research Centre, QUT.txt\",\n          \"Commonwealth Bank of Australia.txt\",\n          \"ANZSA.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 247,\n        \"samples\": [\n          \"Discussion Paper Submission\\nSafe and Responsible AI in Australia | 4 August 2023\\nRhyle Simcock, Nicholas Godfrey, A/Professor Anna Huggins, Professor Mark Burdon\\nSchool of Law/Digital Media Research Centre, Queensland University of Technology*\\nOverview\\nWe thank the Department of Industry, Science and Resources for the opportunity to make a submission in response to the Safe and Responsible AI in Australia discussion paper. We recognise the range of potential advantages that the adoption of AI can bring to both the public and private sectors. However, appropriate governance mechanisms are critical to ensure the responsible development of AI and to mitigate the accompanying risks.\\nAccordingly, Australia\\u2019s existing regulatory frameworks will need to evolve to accommodate the novel challenges posed by AI, and to ensure that adequate mechanisms are in place to address potential harms associated with its use.\\nThis submission draws on our combined expertise as researchers in privacy, public law and digital technologies to outline a number of regulatory reform recommendations for safe and responsible AI in Australia. Our main recommendations, related to questions 1, 2, 6, 7 & 9 of the discussion paper, are that the Australian Government should:\\n\\u2022 adopt different approaches to the regulation of AI in the public and private sectors.\\n\\u2022 amend legislation to safeguard the availability of judicial review for fully or partially\\nautomated decisions.\\n\\u2022 better understand the role of legal code in the digital compliance structures that will\\npower AI/ML developments in the public and private sectors.\\n\\u2022 introduce a statutory requirement for human oversight and review for public sector\\nautomated decision-making systems with serious consequences for individuals.\\n\\u2022 consider approaches to harmonising data quality standards across all levels of\\ngovernment.\\n1\\n\\u2022 consider the merits of design methodologies that promote transparency at all stages\\nof AI development.\\n\\u2022 reform federal and state freedom of information legislation, as well as government\\nprocurement practices, to facilitate open government ideals.\\nQ1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?\\nWe do not contribute specifically to the discussion on core definitions except to add that the adoption of language incorporated internationally through the ISO standard appears to be a sensible way forward and should assist to align Australia with developments in other jurisdictions. Given the increasing standardisation of core concepts through legislation with extra-territorial reach, such as the EU\\u2019s General Data Protection\\nRegulation (\\u2018GDPR\\u2019) framework, it would seem beneficial to adopt generally accepted definitions, rather than unique Australian constructions. However, it is important to note that the discussion paper solely focuses on definitions involving core technical processes of AI/machine learning (ML) without consideration of how different types of data/information could be used in those processes. Most notable is the absence of any definitional considerations of personal or sensitive information which is obviously relevant to the application of AI/ML systems. The current Attorney General\\u2019s Department\\nReview of the Privacy Act (\\u2018AG Review\\u2019) is examining whether the definition of personal and sensitive information should be more aligned with the conceptual basis of the GDPR.\\nIt seems non-controversial, but nonetheless important, to consider core definitions for the range of information that will power AI/ML systems and ensure there is consistency across existing and future legal regimes.\\nQ2. What potential risks from AI are not covered by Australia\\u2019s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?\\n2\\nAs noted in our response above, the discussion paper focuses squarely on the core conceptual processes of AI/ML and the risks arising from generated outputs. The paper tacitly acknowledges that the advent of new AI/ML will require major restructuring of data sharing and availability practices. Current processes are governed by a range of different legal frameworks as outlined in the paper. However, the paper does not engage deeply with the major information risks that could flow from widespread adoption of AI/ML processes and the concomitant requirements of industrialised, or government-wide, data sharing requirements. A major concern is information privacy and how the Australian\\nPrivacy Principles could apply to AI/ML driven technological structures. The AG Review appears to be suggesting stronger alignment between the Privacy Act 1988 (Cth) and the GDPR.\\nWe are strongly supportive of this alignment and repeat calls to better understand how alignment with the GDPR, and the possibility of broader alignments with other EU frameworks, such as the AI Act, could operate in Australia given the absence of an ostensible foundation of fundamental rights that guarantee stronger legal protections for\\nAustralian citizens.1 It is clear that the Australian Government sees AI/ML as a core part of Australia\\u2019s future digital economy and the digital governance structures it requires.\\nThese are by necessity driven by the types of technological components highlighted in the discussion paper. However, while these technical changes are necessary, they should not detract from the deeper and more critical issue of how Commonwealth law should best protect the fundamental rights of Australian citizens.\\nApplications involving the use of AI/ML also need to consider how both public and private sector organisations comply with legal and regulatory obligations, particularly in real- time workflows that produce automated forms of decision-making. Since 2018, we have\\n1\\nMark Burdon and Tegan Cohen (2023) Submission to the Attorney General's Department Privacy Act\\nReview Report https://eprints.qut.edu.au/242023/.\\n3\\nproduced research that outlines the complex challenges arising from the conversion of natural language legal and regulatory obligations into machine executable code that is intended to be used for digital compliance purposes.2 Our research has shown that it is a complex and challenging task to produce machine executable legal code that is both functional from a business use perspective and aligns with legal expectations involving judicially approved processes of statutory interpretation.3 Digital compliance processes predicated on the application of legal code in automated decision-making (\\u2018ADM\\u2019) systems will underpin the safe and responsible development of AI/ML in a more ubiquitous sense. The discussion paper is largely silent on this essential issue but it is nonetheless important to consider especially in relation to the development of new legal and regulatory structures, whether they be regulatory or self-regulatory in nature. A broader conceptual framing is required that considers how the technical components outlined in the discussion paper operate in current and future structures that are governed increasingly by automated forms of output built on legal code. As such, it is important to understand how digital compliance processes are conducted across the different logics and perspectives of legal, regulatory and computational based disciplines and professions.4\\nQ6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?\\n2\\nSee, eg, Anna Huggins, Mark Burdon, Alice Witt and Nicolas Suzor, \\u2018Digitising Legislation: Connecting\\nRegulatory Mind-Sets and Constitutional Values\\u2019 (2022) 14(2) Law, Innovation and Technology 325; Alice\\nWitt, Anna Huggins, Guido Governatori and Joshua Buckley, \\u2018Encoding Legislation: A Methodology for\\nEnhancing Technical Validation, Legal Alignment and Interdisciplinarity\\u2019 (2023) Artificial Intelligence and\\nLaw, https://doi.org/10.1007/s10506-023-09350-1; Anna Huggins et al, \\u2018Submission No 196 to the Select\\nSenate Committee on Financial Technology and Regulatory Technology\\u2019 (Issues Paper Submission, 2020, https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Financial_Technology_and_Regulatory\\n_Technology/FinancialRegulatoryTech/Submissions); Anna Huggins, \\u2018Addressing Disconnection: Automated\\nDecision-Making, Administrative Law and Regulatory Reform\\u2019 (2021) 44(3) University of New South Wales\\nLaw Journal 1048.\\n3\\nMark Burdon et al, \\u2018From Rules as Code to Mindset Strategies and Aligned Interpretive Approaches\\u2019 (2023)\\nJournal of Cross-Disciplinary Research into Computational Law (forthcoming).\\n4\\nHuggins et al, \\u2018Digitising Legislation\\u2019 (n 2 above); Burdon et al (n 3 above).\\n4\\nWe believe that fundamental differences between the public and private sector necessitate different regulatory approaches to the use of AI. While private sector entities typically prioritise profitability, innovation, efficiency and corporate secrecy, traditional public sector principles of good administration include transparency, accountability, rationality, fairness and consistency. Government agencies are not subject to market forces and consumer choice like private entities. Consumers have the power to choose to engage with a different business for the delivery goods and services. However, there is no such freedom of choice in the delivery of government services. The potential risks from the misuse of AI can be heightened in the public sector. There is a significant power imbalance between the state and its citizens, and government use of AI can significantly impact the rights, interests and expectations of individuals. This warrants a higher degree of scrutiny and oversight of the public sector use of AI.\\nAustralia\\u2019s public law frameworks provide important mechanisms for regulating public sector use of AI, including safeguarding executive accountability and protecting individual rights and interests. However, regulatory reform is needed to address gaps in the application of\\nAustralia\\u2019s existing public law frameworks to ADM systems, including to safeguard the contestability of automated government decisions and to ensure human involvement in certain high stakes government decisions.\\nSafeguarding the Contestability of Automated Government Decisions\\nThere is some uncertainty about whether automated government decisions are judicially reviewable. The majority in Pintarich v Deputy Commissioner of Taxation held that a\\n\\u2018decision\\u2019 under the Administrative Decisions (Judicial Review) Act 1977 (Cth) (\\u2018ADJR Act\\u2019) requires a mental process of deliberation.5 This casts doubt on the availability of judicial review under the ADJR Act because ADM, by its very nature, lacks the requisite human mental processes to satisfy this criterion.6 This creates an unacceptable risk that individuals\\n5\\nPintarich v Deputy Commissioner of Taxation (2018) 262 FCR 41.\\n6\\nSee Yee-Fui Ng and Maria O\\u2019Sullivan, \\u2018Deliberation and Automation: When Is a Decision a \\u201cDecision\\u201d?\\u2019\\n(2019) 26(1) Australian Journal of Administrative Law 21.\\n5\\nadversely affected by erroneous or unlawful ADM systems will have limited options for redress.7\\nWe suggest that reform to clarify this legal uncertainty should be a priority for the Australian\\nGovernment. One option is to amend the definition of a \\u2018decision\\u2019 in the relevant State and\\nCommonwealth ADJR Acts to include decisions that are wholly or partly automated.8\\nAnother approach is to modify specific legislation authorising ADM to clarify the availability of judicial review. For example, a deeming provision could be inserted into the respective legislation to confirm that any ADM decision is considered a decision of the authorised decision-maker. We suggest that the first option is preferable given it is inclusive and does not require modification to individual pieces of authorising legislation.9\\nStatutory Protections for Human Involvement in Government Decision-making\\nExisting public law frameworks presuppose that humans remain at the core of the decision- making process. Human oversight and intervention is important for identifying and addressing problems in administrative processes. This remains true for the use of automated systems by administrative agencies. Manual human reviews of the automated debt notices under the Centrelink Online Compliance system (commonly known as\\n\\u2018robodebt\\u2019), for example, would likely have ameliorated many of the problems that arose during its operation. However, there are currently no statutory protections in place that require humans to oversee and review automated outputs.\\nWe suggest that there should be legislative mechanisms in place that mandate human involvement for certain types of automated administrative processes, particularly decisions which have potentially serious consequences for individuals. This aligns with previous recommendations by the Australian Law Reform Commission and the Australian Human\\n7\\nHuggins, \\u2018Addressing Disconnection' (n 2 above) 1061\\u20131064.\\n8\\nYee-Fui Ng et al, \\u2018Revitalising Public Law in a Technological Era: Rights, Transparency and Administrative\\nJustice\\u2019 (2020) 43(3) University of New South Wales Law Journal 1041, 1066.\\n9\\nIbid.\\n6\\nRights Commission that suggest exploring \\u2018the degree of human involvement, if any, that should be required for particular types of decisions\\u2019.10 Some guidance can be sought from the GDPR. Article 22, in particular, prohibits solely automated decision-making that affects individual rights and interests by requiring \\u2018meaningful\\u2019 human involvement. 11 While there are exceptions, minimum protections are in place to ensure that affected individuals have a right to obtain human intervention, to express their views or to contest automated decisions.12\\nQ7. How can the Australian Government further support responsible AI practices in its own agencies?\\nHarmonised Data Quality Standards\\nThe Australian Government should consider approaches to harmonising data quality standards across government, including its collection, management and use. The public sector use of ADM will become increasingly dependent on the exchange of data between administrative agencies. But Australia lacks unified data quality standards across Federal,\\nState and Local Government agencies, including standard definitions, units of measurement and accuracy benchmarks.13 Administrative agencies therefore often have different approaches to how data is organised, characterised and structured.14 This kind of data\\n10\\nAustralian Law Reform Commission, The Future of Law Reform: A Suggested Program of Work 2020\\u201325\\n(Report, December 2019) 24; Australian Human Rights Commission, \\u2018Human Rights and Technology\\u2019 (Final\\nReport, 1 March 2021) 71.\\n11\\nRegulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the\\nProtection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of\\nSuch Data, and Repealing Directive 95/46/EC (\\u2018GDPR\\u2019) [2016] OJ L 119/1 arts 22(2)(a)\\u2013(c). See also Article\\n29 Data Protection Working Party, \\u2018Guidelines on Automated Individual Decision-Making and Profiling for the\\nPurposes of Regulation 2016/679\\u2019 (Guidelines No WP251rev.01, 6 February 2018) 20-21\\n.\\n12\\nGDPR (n 11 above) art 22(3).\\n13\\nData Availability and Use: Productivity Commission Inquiry Report (Australian Government, Productivity\\nCommission, March 2017) 159\\u2013164.\\n14\\nPublic sector data is often fragmented, meaning administrative agencies rarely have enough data to accurately model an outcome of interest in the absence of inter-agency data sharing: Fola Malomo and\\nVania Sena, \\u2018Data Intelligence for Local Government? Assessing the Benefits and Barriers to Use of Big\\nData in the Public Sector\\u2019 (2017) 9(1) Policy & Internet 7, 9\\u201310. See also Andrew Iliadis, \\u02bbThe Tower of Babel\\nProblem: Making Data Make Sense with Basic Formal Ontology\\u02bc (2019) 3(6) Online Information Review\\n1021.\\n7\\nfragmentation can have detrimental impacts on the quality of the data, and consequently, the validity of ADM outputs.\\nWe note that the Data Availability and Transparency Act 2022 (Cth) introduced a legislative scheme for sharing Australian Government data. However, the Act does not contain any stipulations regarding data quality. Only the Australian Privacy Principles, enacted through the Privacy Act 1988 (Cth), impose an obligation on all private and public sector entities to ensure the quality and accuracy of information they hold.15\\nOne potential solution is for the Office of the National Data Commissioner, as the national regulator for Australian Government data sharing, to provide advice and guidance on inter- agency data quality standards. This might include, for example, technical best practice for the collection, management and use of government data nationwide.\\nQ9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:\\na. where and when transparency will be most critical and valuable to mitigate\\npotential AI risks and to improve public trust and confidence in AI.\\nb. mandating transparency requirements across the private and public\\nsectors, including how these requirements could be implemented.\\nTransparency by Design\\nWe recommend consideration of design methodologies that promote transparency at all stages of development. In particular, the Government should consider:\\na) mandating technical and organisational record-keeping across the AI lifecycle.\\nContemporary technical and interdisciplinary scholarship has developed a suite of\\nvaluable tools for recording and disclosing information at different stages of the AI\\n15\\nPrivacy Act 1988 (Cth), sch 2 s 10.\\n8\\nlifecycle. This includes data documentation and provenance methods,16 model\\nperformance cards,17 auditing processes and logging mechanisms.18 Organisational\\nrecord-keeping is equally as important. Documentation about impact assessments,\\nprocurement processes and broader organisational choices should also be publicly\\navailable for scrutiny.19\\nb) adopting inherently transparent and interpretable AI systems in high-stakes settings.\\nAI complexity is a significant impediment to meaningful transparency.20 Such\\ninscrutability is often a symptom of system design.21 The more sophisticated kinds of\\nmachine learning models are not always necessary to achieve organisational goals.\\nNor do more complex models necessarily lead to better predictive performance.22 It is\\noften possible to achieve good predictive performance with much simpler machine\\nlearning models.23 Accordingly, simpler and inherently transparent AI systems should\\nbe used in domains where automated outputs will have significant consequences for\\naffected individuals.\\nc) consider the merits of acceptance test driven development (\\u2018ATDD\\u2019) methodologies.\\nATTD approaches, such as the development of interdisciplinary \\u2018user stories\\u2019, can\\nclose the understanding gap between policy designers and developers of ADM\\nsystems, while also generating concise natural language documentation which can\\nconvey the intent of specific system operations. While ethical guidelines can be useful,\\n16\\nTimnit Gebru et al, \\u2018Datasheets for Datasets\\u2019 [2018] arXiv:1803.09010 [cs]\\n.\\n17\\nMargaret Mitchell et al, \\u2018Model Cards for Model Reporting\\u2019 (Conference Paper, Conference on Fairness,\\nAccountability, and Transparency, 2019) .\\n18\\nJoshua A Kroll, \\u2018Outlining Traceability: A Principle for Operationalizing Accountability in Computing\\nSystems\\u2019 (Conference Paper, Conference on Fairness, Accountability, and Transparency, 2021)\\n; Jatinder Singh, Jennifer Cobbe and Chris Norval,\\n\\u2018Decision Provenance: Harnessing Data Flow for Accountable Systems\\u2019 (2019) 7 IEEE Access 6562.\\n19\\nJennifer Cobbe, Michelle Seng Ah Lee and Jatinder Singh, \\u2018Reviewable Automated Decision-Making: A\\nFramework for Accountable Algorithmic Systems\\u2019 (Conference Paper, Conference on Fairness,\\nAccountability, and Transparency, 2021) .\\n20\\nJenna Burrell, \\u2018How the Machine \\u201cThinks\\u201d: Understanding Opacity in Machine Learning Algorithms\\u2019 3(1)\\nBig Data & Society 1, 4\\u20135.\\n21\\nJoshua A Kroll, \\u2018The Fallacy of Inscrutability\\u2019 (2018) 376(2133) Philosophical Transactions of the Royal\\nSociety of London 20180084.\\n22\\nCynthia Rudin, \\u2018Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use\\nInterpretable Models Instead\\u2019 (2019) 1(5) Nature Machine Intelligence 206.\\n23\\nIbid.\\n9\\nAI developers cannot always map principles to practical development.24\\na. Ethical User Stories25 have been proposed to increase alignment of ADM\\nsystems26 by outlining links between abstract ethical values and functional\\nrequirements.27 A preliminary study comparing the use of ECCOLA,28 a\\nmethodology for ethical user story construction, with standard user story\\napproaches, revealed that the former produced noticeably more \\u2018human-\\ncentric\\u2019 user stories, while standard user story methods generated significantly\\nmore \\u2018technology-centric\\u2019 user stories.29 The study further showed that, when\\ncompared to an independent user story evaluation model,30 the ECCOLA\\nmethod produced higher-scoring stories, indicating that even beyond ethical\\nconsiderations, there is value for developers and other stakeholders in\\nadoption of the method.31\\nb. Value sensitive design32 methodologies such as participatory design have\\nbeen increasingly explored to embed desired values into AI systems.33 These\\napproaches reduce the likelihood of marginalised groups being unfairly\\nimpacted.34 They have been used to shift the design of legal AI systems\\u2019 goals\\n24\\nVille Vakkuri et al, \\u2018ECCOLA \\u2014 A Method for Implementing Ethically Aligned AI Systems\\u2019 (2021) 182\\nJournal of Systems and Software 111067; Erika Halme et al, \\u2018How to Write Ethical User Stories? Impacts of the ECCOLA Method\\u2019 in Peggy Gregory et al (eds), Agile Processes in Software Engineering and Extreme\\nProgramming (Springer International Publishing, 2021) 37.\\n25\\nEthical user stories differ from standard user stories in that the former also considers non-functional user requirements: Erika Halme et al, \\u2018Ethical User Stories: Industrial Study\\u2019 in Joint Proceedings of REFSQ-2022\\nWorkshops, Doctoral Symposium, and Posters & Tools Track (CEUR-WS, 2022) 5.\\n26\\nIbid; Halme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 36.\\n27\\nQinghua Lu et al, \\u2018Towards a Roadmap on Software Engineering for Responsible AI\\u2019 in Proceedings of the\\n1st International Conference on AI Engineering: Software Engineering for AI (Association for Computing\\nMachinery, 2022) 101, 105 .\\n28\\nVakkuri et al (n 24 above).\\n29\\nHalme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 45-46, 49.\\n30\\nLuigi Buglione and Alain Abran, \\u2018Improving the User Story Agile Technique Using the INVEST Criteria\\u2019 in\\n2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th\\nInternational Conference on Software Process and Product Measurement (2013) 49.\\n31\\nHalme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 46-47, 49-50.\\n32\\nBatya Friedman, \\u2018Value-Sensitive Design\\u2019 (1996) 3(6) Interactions 16.\\n33\\nSteven Umbrello, \\u2018Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design\\nApproach\\u2019 (2019) 3(1) Big Data and Cognitive Computing 5.\\n34\\nSee, eg, Q Vera Liao and Michael Muller, \\u2018Enabling Value Sensitive AI Systems through Participatory\\nDesign Fictions\\u2019 (No arXiv:1912.07381, arXiv, 12 December 2019) .\\n10\\nand methods from computer scientists to legal experts, as well as decrease\\nthe knowledge gaps between such domain experts.35 Further, participatory\\ndesign has a history of being used in the development of expert systems to\\nreduce abstraction,36 thereby encouraging the development of decision-\\nmaking systems which can be understood with limited computational\\nproficiency. Participatory design can be used across various points of\\ndevelopment.37 Like ethical user story approaches, they generate artifacts\\nwhich are collaboratively constructed by computer scientists and domain\\nexperts alike.38 Accordingly, they can be used to audit and understand the\\ngoals and methods of ADM systems. In this way, the implementation of ethical\\nuser stories or participatory design methodologies generates transparency as\\na by-product, and therefore may be palatable to developers concerned about\\ninefficiencies and the costs of compliance with transparency mandates.\\nSafeguarding Transparency in the Government Use of AI\\nTransparency and explainability are critically important for all uses of AI, but they are especially crucial in the public sector where there is an expectation of higher standards of transparency and accountability. AI systems used in the public sector should be sufficiently transparent to permit public scrutiny and facilitate contestation where necessary. In line with open government and transparency ideals, we recommend introducing the following regulatory reform solutions aimed at tackling AI opacity barriers in the public sector:\\na) Reforming federal and state freedom of information legislation to ensure that\\nsufficient information about Government AI can be obtained by individual citizens,\\nmedia and civil society groups. Government refusal of some freedom of information\\n35\\nFernando Delgado, Solon Barocas and Karen Levy, \\u2018An Uncommon Task: Participatory Design in Legal\\nAI\\u2019 (2022) 6(CSCW1) Proceedings of the ACM on Human-Computer Interaction 51:1-51:23, 51:2, 51:5.\\n36\\nIbid 51:5.\\n37\\nDouglas Zytko et al, \\u2018Participatory Design of AI Systems: Opportunities and Challenges Across Diverse\\nUsers, Relationships, and Application Domains\\u2019 in Extended Abstracts of the 2022 CHI Conference on\\nHuman Factors in Computing Systems (Association for Computing Machinery, 2022) 1, 2\\n (\\u2018Participatory Design of AI Systems\\u2019).\\n38\\nJeanette Blomberg, Lucy Suchman and Randall H Trigg, \\u2018Reflections on a Work-Oriented Design Project\\u2019\\n(1996) 11(3) Human\\u2013Computer Interaction 237.\\n11\\nrequests was identified as a significant impediment to meaningful scrutiny of the\\nCentrelink online compliance intervention scheme.39\\nb) Establishing public sector procurement standards that prioritise transparency over\\ncommercial secrecy and outline what specific information administrative agencies are\\nrequired to collect and disclose about AI systems in use.40 These standards should\\nalso be used to facilitate openness in freedom of information requests when AI-\\nsystems are developed by, or in conjunction with, private contractors.\\nc) Instituting a proactive disclosure regime containing a public register of Government\\nAI systems in use, as well as public disclosure of technical documentation sufficient\\nto facilitate external scrutiny and individual contestation. Ideally, government\\nagencies should be required to make the source code of AI publicly available. We\\nrecognise, however, that source code transparency alone may be insufficient for\\nachieving meaning accountability and transparency in AI systems,41 particularly for\\nadversely affected individuals.42 Therefore, in addition to source code disclosure,\\ntechnical and organisational documentation of coding decisions should also be made\\npublicly available.\\nd) Modifications to the ADJR Act that explicitly require the provision of a statement of\\nreasons for all decisions made by, or in reliance on, an ADM system. Reasons are\\nessential for the contestation of automated administrative decisions. Affected\\npersons should be able to understand the rationality behind a decision and be given\\nenough information to contest it where necessary. However, we note that there is\\nsignificant doubt about the ability for fully automated AI systems to produce \\u2018legally\\n39\\nSee Ashlynne McGhee, \\u2018Centrelink Debt Recovery Program: Department Rejects FOI Requests Relating to Plagued Scheme\\u2019, ABC News (online, 10 February 2017) .\\n40\\nCatherine Holmes, Royal Commission into the Robodebt Scheme (Report, 2023) 656\\u2013657.\\n41\\nJoshua A Kroll et al, \\u2018Accountable Algorithms\\u2019 (2016) 165(3) University of Pennsylvania Law Review 633,\\n647\\u2013650.\\n42\\nSee, eg, Mike Ananny and Kate Crawford, \\u2018Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability\\u2019 (2018) 20(3) New Media and Society 973; Deven R Desai and Joshua A Kroll, \\u2018Trust but Verify: A Guide to Algorithms and the Law\\u2019 (2017) 31(1) Harvard Journal of\\nLaw and Technology 1.\\n12\\ncompelling\\u2019 reasons for decisions.43 Technical explanations, including those\\nproduced by modern explainability tools, often fall short of providing the type of\\nreasons needed to facilitate understanding and contestation of an administrative\\ndecision. Reasons must go beyond the logic of how the decision was made and\\ninclude reasons as to why the decision was made.44 In circumstances where this is\\nnot possible, we suggest that ADM should not be used for government decision-\\nmaking purposes.\\n* About QUT\\u2019s Digital Media Research Centre\\nThe Digital Media Research Centre (\\u2018DMRC\\u2019) at the Queensland University of Technology is a leading research centre in digital humanities and social science research with a focus on digital communication, media, and the law. Our research programs investigate digital inclusion and participation, the digital transformation of media industries, the growing role of\\nAI and automation on digital societies and the role of social media in public communication.\\nScholars in the DMRC are undertaking important research on many of the concerns raised by the discussion paper. This includes ongoing work to investigate the development of regulatory regimes to address emerging risks for AI and ADM.\\n43\\nWill Bateman, \\u2018Algorithmic Decision-Making and Legality: Public Law Dimensions\\u2019 (2020) 94(1) Australian\\nLaw Journal 520, 527.\\n44\\nJennifer Cobbe, \\u2018Administrative Law and the Machines of Government: Judicial Review of Automated\\nPublic-Sector Decision-Making\\u2019 (2019) Legal Studies 1, 22.\\n13\",\n          \"\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\",\n          \"About us\\n1 This submission is made on behalf of the Australia New Zealand Screen Association (ANZSA).\\nThe ANZSA represents the film and television content and distribution industry in Australia and\\nNew Zealand. Its core mission is to advance the business and art of filmmaking, increasing its\\nenjoyment around the world and to support, protect and promote the safe and legal\\nconsumption of movie and TV content across all services. Members of ANZSA include: Village\\nRoadshow Limited; Motion Picture Association; Walt Disney Studios Motion Pictures; Netflix\\nInc.; Paramount Pictures; Sony Pictures Releasing International Corporation; Universal\\nInternational Films, Inc.; and Warner Bros. Pictures International, a division of Warner Bros.\\nEntertainment Inc., and Fetch TV.\\nGeneral comments\\n2 We thank the Government of Australia and the Department of Industry, Science and Resources\\nfor the opportunity to provide comments on the discussion paper titled Safe and responsible AI\\nin Australia (\\u201cthe discussion paper\\u201d). We welcome the Department\\u2019s forward-looking approach\\nin opening the consultation, and for the opportunity for interested parties like ANZSA and our\\nmembers to share our input. We encourage the Government to continue providing frequent and\\ntransparent opportunities for public input on the topic of artificial intelligence (\\u201cAI\\u201d)1 regulation,\\nso that interested stakeholders can continue providing feedback on this pressing and important\\nissue. We also urge the Government not to rush to regulate AI or to impose new and hasty\\nrules on the use of AI. Further development of regulation, if and when there is a demonstrated\\nneed to regulate, should be done in close consultation with all relevant stakeholders, including\\ninterested industry representatives.\\n3 We note the acknowledgement in the discussion paper that \\u201cthe range of contexts in which AI\\ncan be used, and for different purposes, may necessitate context-specific resources\\u201d, and that\\nthe consultation \\u201cdoes not seek to consolidate or replicate the development of existing general\\nor sector-specific regulations and governance initiatives across the Australian Government.\\u201d\\nAny broad new AI regulation with application across multiple sectors will also run the risk of\\noverlapping with existing regulation, causing regulatory uncertainty, which would create a\\nheavy compliance burden and disincentivize business investment.\\n4 Governments should consider carefully, and in consultation with relevant stakeholders and\\ninformed by an assessment of risk and potential for harm, the need for a regulatory framework\\nfor AI. If such a framework is found to be necessary, any obligations must be proportionate to\\nthe potential for harm. Any burdensome requirements will add another layer of compliance\\n1 We note that AI is a term used broadly but covers many technologies. Generative AI refers to a subset of artificial\\nintelligence that learns patterns from data and, when directed by a person through \\u201cprompting\\u201d, produces content based\\non those patterns. This contrasts with \\u201ctraditional AI\\u201d systems that are used to predict outcomes or generate insights.\\nburden on companies and could have major unintended consequences, including providing a\\nstrong disincentive to further investment for content creators. Broadly speaking, the creative\\nsector flourishes best in a context of light-touch regulation that encourages ease of doing\\nbusiness, both domestically and internationally.\\n5 In this regard, we support the Government\\u2019s consideration of a risk management approach that\\n\\u201ccaters to the context-specific risks of AI\\u201d and \\u201callows for less onerous obligations for lower risk\\nAI uses\\u201d. We note (in Box 4, page 32 of the discussion paper) that the Government considers\\nthe use of \\u201cAI-enabled recommendation engines to enable personalised online shopping\\nrecommendations based on users\\u2019 browsing history, preferences and interests\\u201d as a low-risk\\nuse case, with a \\u201climited, reversible or brief\\u201d impact that should allow for fewer or no obligations\\nfor its use.\\nAI can support human creativity\\n6 ANSZA member companies are already using or plan to use AI to support the creation and\\ndelivery of a wide range of works that bring benefits (both economic and cultural) to society.\\nRecent advances in AI technology, including the rise of generative AI tools like Midjourney and\\nDall-E, are leading to major and complex debates around the copyright implications of this\\ntechnology; and to intensifying pressure to regulate the use of AI, both in Australia and in other\\njurisdictions around the world. However, we note that notwithstanding these substantial\\ndevelopments, the AI field is still in a relatively nascent stage \\u2013 there are still further\\ndevelopments to come.\\n7 AI is an enabling tool that can complement aspects of filmmaking process, the audience\\nviewing experience, and fan engagement. We would note that the use of AI is not novel and\\nhas been employed as a tool in the production process, particularly in the context of special\\neffects. For example, \\u201ctraditional AI\\u201d has been used in a number of ways in production, such as\\nto predict resource usage, optimization of shooting schedules, and predicting complexity of\\nVFX shots. AI is also used in fairly routine post-production work like colour correction, detail\\nsharpening, de-blurring, or removing unwanted objects. Some are more involved, like aging\\nand de-aging an actor.\\n8 Across each of these examples, it is clear that AI use cases across the screen industry are\\ninherently low risk to consumers. They do not present the same level of harm as generating\\nmanipulated media for the purposes of misleading or deceiving a consumer. Instead, it is a\\nvaluable tool in the production process that is becoming increasingly important to present\\nvisually compelling experiences for audiences.\\n9 The rapid availability of generative AI has added additional layers of possibility as well as\\ncomplexity. Notably, we do not believe it will replace human creativity. Instead, ANZSA\\nmembers believe AI and generative AI will serve to free up humans from the most rote parts of\\ntheir work, allowing them to concentrate their limited time and effort on the most creative\\naspects.\\nDifferentiating the use of AI in curated VOD services versus user generated content\\nservices\\n10 Several ANZSA members operate, or are planning to operate, video-on-demand (VOD)\\nservices in Australia which offer large and diverse catalogues with human-curated and\\nprofessionally produced content. This human curation element is crucial; ANZSA\\u2019s members\\ncomply with existing regulation to ensure that viewers are appropriately informed about the\\ncontent they are about to watch, or which they allow their children to watch. These VOD\\nservices use various recommendation systems (some of which could be enhanced by AI) to\\nhelp viewers find content that most closely suits their interests. ANZSA submits that the use of\\nAI within this framework is inherently low risk.\\n11 This contrasts with user generated content services which do not curate their content offering\\nand as a result the risk of adverse outcomes by purely AI-driven recommendation processes is\\ngreater in this situation. ANZSA submits that regulation should recognise the differentiation\\nbetween VOD services and providers of user generated content in terms of content\\nresponsibility, consumer interest and creative-led freedom of expression, particularly with\\nrespect to public policy concerns about the potential use of AI in promoting misinformation and\\ndistributing harmful content via digital platforms.\\n12 The recommendation systems used in VOD services pose little or no risk of the type that\\nwarrant regulation. VOD services\\u2019 recommendation engines should therefore clearly be\\nconsidered very low-risk uses of AI, with few or no obligations imposed on their use.\\nAI and Copyright\\n13 ANZSA members, and the creative community more broadly, rely on strong and effective\\ncopyright legislation and policy to protect their production of and investment in creative content,\\nwhich is enjoyed around the world. Such copyright policy is of utmost importance to the creative\\ncommunity and requires considerable attention from the relevant experts, especially given the\\nhost of issues that AI has brought up in relation to intellectual property.\\n14 As the discussion paper notes, the Attorney-General\\u2019s Department (AGD) is already in the\\nprocess of organizing Ministerial Roundtables on copyright, including an upcoming one\\nscheduled for August 2023 on the implications of AI for copyright law. Given the sectoral\\nexpertise of the AGD, copyright policy should fall under its remit and the process coming out of\\nthe Ministerial Roundtables. ANZSA and its members are currently participating actively in the\\nMinisterial Roundtables.\\n15 This AI consultation led by the Department of Industry, Science and Resources should have a\\nbroader remit addressing other issues. Should a report be released following this consultation,\\nit should also note that copyright-related AI issues fall outside the Department\\u2019s purview and\\nwill be addressed by the AGD.\\n16 We thank the Department again for providing the opportunity to provide our comments on this\\nimportant topic. We are happy to meet to discuss these comments.\\nPaul Muller\\nCEO Australia New Zealand Screen Association\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export df_files to an Excel file\n",
        "if df_files is not None:\n",
        "    excel_filepath = \"/content/drive/MyDrive/submission_content.xlsx\"  # Replace with desired path\n",
        "    df_files.to_excel(excel_filepath, index=False)  # Set index=False to avoid writing row indices\n",
        "    print(f\"DataFrame exported to: {excel_filepath}\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_files' is empty or None. Cannot export.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWzwfuksIVho",
        "outputId": "5a5d283a-c01f-402e-d089-7ee9ac3d87f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame exported to: /content/drive/MyDrive/submission_content.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_transparency_text(df_files):\n",
        "  \"\"\"\n",
        "  Extracts text related to transparency from the 'content' column of a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df_files: A pandas DataFrame with a 'content' column containing text.\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame with an additional 'transparency_text' column containing the extracted text.\n",
        "    Returns the original DataFrame if 'df_files' is None or doesn't have a 'content' column.\n",
        "  \"\"\"\n",
        "\n",
        "  if df_files is None or 'content' not in df_files.columns:\n",
        "      return df_files\n",
        "\n",
        "  transparency_texts = []\n",
        "  for content in df_files['content']:\n",
        "      # Use regular expressions to find sections related to transparency\n",
        "      # This regex looks for text containing \"transparency\",  question 9 or variations.\n",
        "      matches = re.findall(r\"(?:transparency|9\\.\\s*(?:Given|importance)|mitigate potential AI risks|public trust and confidence in AI|mandating transparency requirements)(?:.*?)(?=(?:transparency|9\\.|mitigate potential AI risks|public trust and confidence in AI|mandating transparency requirements)|\\Z)\", content, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "      # Combine all matches for each file into a single string.\n",
        "      extracted_text = \" \".join(matches)\n",
        "      transparency_texts.append(extracted_text)\n",
        "\n",
        "  df_files['transparency_text'] = transparency_texts\n",
        "  return df_files\n",
        "\n",
        "df_files = extract_transparency_text(df_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bsHbizG_I4nP",
        "outputId": "351a549e-6492-45a1-d926-6c5df8d9d94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              filename  \\\n",
            "0                        Australian Federal Police.txt   \n",
            "1    Office of the Australian Information Commissio...   \n",
            "2                         Law Council of Australia.txt   \n",
            "3                Digital Industry Group Inc (DIGI).txt   \n",
            "4               Department of Health and Aged Care.txt   \n",
            "..                                                 ...   \n",
            "242                                            OOP.txt   \n",
            "243                                     Complexico.txt   \n",
            "244                                Lilin Australia.txt   \n",
            "245                                      IdeaSpies.txt   \n",
            "246                           Codemaster Institute.txt   \n",
            "\n",
            "                                     transparency_text  \n",
            "0    transparency, accountability, fairness, privac...  \n",
            "1    public trust and confidence in AI.4 The OAIC h...  \n",
            "2    transparency.....................................  \n",
            "3    transparencyreport.google.com/youtube-policy/r...  \n",
            "4    transparency of AI in the delivery of health c...  \n",
            "..                                                 ...  \n",
            "242                                                     \n",
            "243  transparency, accountability, and trust in the...  \n",
            "244                                                     \n",
            "245                                                     \n",
            "246                                                     \n",
            "\n",
            "[247 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_files['transparency_text'].iloc[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JHdIYDznMcZZ",
        "outputId": "ce13cf7c-bf0a-4310-8d44-dd2282bf66e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"transparency of AI in the delivery of health care is essential and this should be consistent across public and private sectors.\\nThe Department supports greater ethics consideration when using data for AI purposes, particularly as it relates to health outcomes to enable maximum benefits while ensuring there is sufficient trust in the outcomes and how it affects individuals and society. For areas with direct impact on health outcomes of individuals, there is less tolerance for risk and the response should be proportional to the possible impact.\\nThe Department supports the current approach by the Therapeutic Goods Administration (TGA) who regulate products that are intended for medical use including software (that incorporates AI), with a robust regulatory framework for software based medical devices. The framework addresses risks associated with AI and applies to any software included with, or that is a part of, a medical device that is used for diagnosis, prevention, monitoring, treatment, alleviation of disease, injury or disability.\\nThe TGA regularly consults on its regulations to ensure it considers emerging technologies (and risks) to ensure the regulations remain fit for purpose and continue to safeguard Australian patients. The TGA publicly consulted on software including AI in 2019 and 2020 – and published updated specific guidance including clinical evidence and performance requirements in early 2021. Further information about the framework and risk classification with some examples is included in the attached Health response\\n[Regulation of Software-based Medical Devices - Info sheet for DISR July 2023].\\nThe Department does not recommend banning the use of high-risk AI applications, rather DISR may consider developing guidance on how to use controls to mitigate risk appropriately. The Department strongly advocates for a risk-based approach in relation to AI and recognises that it may need to be mandatory for moderate to high-risk applications in health and aged care. This provides flexibility to ensure regulatory burden and oversight align with the potential risk of a particular activity, and to reduce burden and promote innovation for low-risk AI applications. Key elements of a risk-based approach should include clear definitions of consequences of the risk and objective, clearly articulated criteria to determine the risk level and how to appropriately deal with the risk. Leveraging existing risk- based approaches, integrating AI-specific risks and controls into risk management, and employing a mix of regulatory and non-regulatory frameworks can support the development of a risk-based approach for addressing AI risks.\\nThe Department recommends DISR considers, in partnership with appropriate regulators such as the\\nNational Data Commissioner and the Information Commissioner, the development of guidelines for\\nData Impact Assessments (DIA) as part of AI assessments. The DIA could be mandatory for organisations applying AI above a set impact threshold, similar to Privacy Impact Assessments. The DIA could take a multi-faceted approach taking into account the purpose, explainability, ethics, sensitivity, sovereignty, security, and impact to provide a holistic assessment of risk and need for regulation.\\nAdditional input and detail on the discussion paper has been provided via direct responses to the\\n20 discussion questions. This input differentiates between issues directly related to the Department in comparison to the broader Australian Healthcare System. i\\nCenter for AI Safety (CAIS) ii\\nhttps://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00077-4/fulltext and https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00237-2/fulltex\\n2|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nSafe and Responsible AI in Australia – Discussion Paper August 2023\\nQuestion Department of Health and Aged Care Australian Healthcare System\\nDEFINITIONS\\n1 Do you agree with the Whilst a high-level definition could be useful, the way AI is used is context specific, as different No objection.\\ndefinitions in this discussion sectors have differing needs.\\npaper? If not, what The Therapeutic Goods Administration (TGA) already has established definitions related to AI for\\ndefinitions do you prefer medical (therapeutic) use which are aligned with the definitions in “Machine Learning-enabled\\nand why? Medical Devices: Key Terms and Definitions” published by the International Medical Device\\nRegulators Forum (IMDRF) in May 2022.\\nThe Australian Commission on Safety and Quality in Health Care (ACSQHC) supports the use of\\nthe International Organisation for Standardization definitions.\\nIt is recommended to also include a definition for Automated Decision Making (ADM) to avoid\\nmisinterpreting ADM as fully autonomous. From a legislation perspective, it is recommended to\\nkeep the definition as technologically neutral as possible and consider future use-cases for AI to\\nhelp maintain relevance.\\n2 What potential risks from AI The Department suggests that there is no clear pathway for the sharing of sensitive unit record In considering the development of AI\\nare not covered by health and aged care data with commercial entities within our current legislative frameworks. regulation we need to consider a wide\\nAustralia’s existing The existing regulations only permit the disclosure of critical departmental data, such as the array of clinicians and professional bodies\\nregulatory approaches? Australian Immunisation Register, Medicare Benefits Scheme, and Pharmaceutical Benefits across the health and aged care sectors and\\nDo you have suggestions for Scheme data, to a limited number of trusted Commonwealth agencies, including the Australian their risk appetites.\\npossible regulatory action to Bureau of Statistics (ABS) and the Australian Institute of Health and Welfare (AIHW). Similar Over the past few years, lots of work has\\nmitigate these risks? disclosures to commercial entities are not permitted under either the primary legislation or the been undertaken to strengthen regulation\\nData Availability and  Transparency Scheme (which does not cover private sector firms). and safeguards of Australia’s critical\\nTo effectively manage risks and protect the integrity of healthcare information while harnessing infrastructure. Due diligence to ensure that\\nthe benefits of AI, the Department advocates for comprehensive regulatory reforms and there is no erosion of other forms of\\nproposes integrating AI-specific regulations within existing Acts. These reforms would legislation would be essential.\\nnecessitate mandatory training for healthcare professionals and adherence to specific AI-related\\nprofessional standards, ensuring that AI is utilised responsibly and ethically within the health\\nsector.\\n1|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nThe Department emphasises the need to implement flagging mechanisms or additional security\\nchecks for medical professionals and researchers seeking access to sensitive data. This would\\nensure accountability and reduce potential security risks associated with the access and use of\\nsensitive health information.\\nAddressing bias in AI algorithms is of utmost importance to avoid disproportionate impacts on\\nvulnerable populations, including First Nations and CALD and LGBTQIA+ and people with a\\ndisability. Data sets on which AI tools are trained, do themselves have inherit bias, (ie male\\nskewed, no comprehensive data on women, gender reduced to the binary) making some groups\\n“invisible” to the algorithm.\\nPrejudice cannot be coded out of a model, but proper representation for minority groups can be\\ntaken into considered in setting up AI modelling. To support the empowerment of First Nations\\ncommunities, The Department stresses the significance of adhering to Priority Reform 4 of the\\nClosing the Gap Agreement. This reform aims to grant First Nations people the ability to collect,\\nanalyse, and use data in meeting their community's unique needs and priorities. Respecting data\\nsovereignty rights and fostering genuine partnerships between the government and First Nations\\npeople are critical principles that must be incorporated into the development of AI technologies\\nand regulatory frameworks. Complying with the CARE principles of Indigenous Data Governance\\nfurther reinforces the commitment to fair and ethical AI practices.\\nHaving approaches for ongoing monitoring and potential re-training of AI technologies and\\nmodels is essential to ensure their relevance and performance over time. Implementation of an\\nAI tool without ongoing review can result in performance deterioration over time due to data\\ndrift, where the data used to train the model is different to the data where the model is being\\napplied. This could have serious implications if the AI tool is being used in a high-risk sector such\\nas healthcare, and evidence suggests it is already a concern in medical machine learning\\ndeployment1. The Department would like the longer-term management and use of AI tools be\\nconsidered at project inception, including responsibilities for managing the tool, and\\nmechanisms for detecting and mitigating data drift and performance degradation.\\nThe Department emphasises the significance of AI applications being available in multiple\\nlanguages, ensuring inclusivity and accessibility for diverse populations, particularly culturally\\nand linguistically diverse (CALD) communities. Addressing potential racial discrimination in AI,\\n1\\nhttps://www.birpublications.org/doi/10.1259/bjr.20220878\\n2|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission particularly concerning recidivism predictions, demands meticulous examination and regulatory intervention to uphold fairness and justice.\\nThe TGA have been regulating products that are intended for medical use including software\\n(that incorporate AI) since 2002, using a robust regulatory framework for software based medical devices. The framework addresses risks associated with AI, and applies it to any software included with, or that is a part of, a medical device that is used for diagnosis, prevention, monitoring, treatment, alleviation of disease, injury or disability. Regulatory requirements are technology agnostic and apply regardless of whether the product incorporates components like AI, chatbots, cloud, mobile apps or other technologies.\\nThe TGA regularly consults on its regulations to ensure it considers emerging technologies (and risks) to ensure the regulations remain fit for purpose and continue to safeguard users. The TGA publicly consulted on software including AI in 2019 and 2020 – and published updated specific guidance including clinical evidence and performance requirements in early 2021. The TGA has also consulted with specific groups such as MSIA and relevant health professional colleges on specific types and uses of software.\\nThe TGA has a range of regulatory actions it takes when software or AI is not performing as intended or if a product is being supplied without appropriate regulatory approval.\\nFurther information about the framework and risk classification with some examples is included in Attachment C of the Departments response [Regulation of Software-based Medical Devices -\\nInfo sheet for DISR July 2023].\\nThe National Mental Health Commission (NMHC) would like to ensure the development and utilisation of AI across Australian society does not result in people who experience mental ill- health being treated unfairly, or in other harms to the mental health and wellbeing of the\\nAustralian community. The Department highlights the specific risks associated with AI for individuals experiencing suicidality. Past incidents where AI inadvertently facilitated access to harmful information underscores the need for vigilance and prompt regulatory action to mitigate potential risks.\\nThe Department notes the discussion paper did not outline regulation regarding the use of AI in the health sector, including in supporting/replacing health workforce and points out there is a risk of AI decision making being relied upon in remote settings versus human decision making supported by AI in urban areas. Ensuring equitable access to healthcare, especially for rural and remote communities, requires the responsible integration of AI support. The Department\\n3|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nrecommends regulations that cover various aspects of AI's impact, including clinical decision-\\nmaking, medical report writing, pathology, and health administration. The Medical Workforce\\nPolicy and Strategy underscores the importance of flexible and adaptable regulations,\\nconsidering the rapid pace of AI technology evolution.\\nThe Australian Digital Health Agency (ADHA) suggests that AI presents a risk to the maintenance\\nof quality healthcare information, which is currently not covered by Australia’s existing\\nregulatory approaches. The maintenance of healthcare information is currently governed by\\nprofessional standards and regulatory frameworks that promote accuracy, quality, and handling\\nof personal health information, such as the My Health Records Act 2012, Healthcare Identifiers\\nAct 2010 and Privacy Act 1988. None of this legislation currently contemplates risks from AI nor\\nthe benefits, and although the My Health Records Act 2012 does allow for decision making using\\na computer program, there is no specific instruction on automated decision-making.\\nThe Government is considering reforms to each of these Acts so there are opportunities to\\ninclude AI-specific regulation as necessary. Current mitigations would be limited to relying on\\nhealthcare professionals undergoing mandatory training and have them meet a set of\\nprofessional standards. Another potential risk from AI that is not covered by Australia’s existing\\nregulatory approaches relates to the use of AI in policy development and administration in the\\nAustralian Public Service. Clarity and disclosure around the use of AI data and algorithms, and\\nthe limitations of these methods, is crucial when AI outcomes are used to develop policy. The\\nAgency welcomes the recently published Interim guidance for agencies on government use of\\ngenerative AI platforms and recommends the development of more detailed guidance in the\\nfuture, particularly in relation to policy development and administration.\\nThe Department suggests establishing nationally agreed AI principles as well as nationally agreed\\n3 Are there any further non- • Likely need for public Education\\nregulatory initiatives the ethical, clinical and technical standards for AI. It is important to develop these national principles campaigns to provide education of the\\nAustralian Government and standards using a transparent, co-designed and consensus-based approach (and leveraging risks and benefits of the use of AI to\\ncould implement to support international standards where appropriate) to support community trust and confidence in AI. It ensure community and clinician\\nresponsible AI practices in acknowledges efforts by the NSW Government in developing an AI Assurance Framework which awareness and trust levels remain high.\\nAustralia? mandates ethical principles to govern bespoke AI systems. It would be appropriate to consider if • Consider establishing controlled\\nPlease describe these and the NSW Framework can be applied nationally. environments for developers to test AI\\ntheir benefits or impacts. systems to identify risks and mitigation\\nThe Department encourages partnerships with academic researchers and centres of excellence,\\nstrategies before AI systems are\\nsuch as the National AI Centre, and close monitoring of the Responsible AI Adopt Program, CSIRO\\nreleased for use by Australians.\\nData61, and UTS Australian AI Institute to facilitate innovation, knowledge sharing, and resource\\nutilisation. It also encourages an approach to AI governance that considers publicly available\\n4|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission information both nationally and internationally. It will be important to learn from international • Suggest encouraging positive incentives examples such as the European Union (EU) and Canadian approach and incorporating for compliance – accountability often international guidelines, standards, and certifications into a national AI framework will ensure build on penalties but incentives to responsible adoption of AI technologies. Supporting AI developers and users is a priority, and reinforce safe and responsible use of AI diligently investigating tools developed by countries like the US and Singapore to identify and could be introduced.\\nmitigate AI-related risks effectively should be considered.\\nThe work of the ACSQHC may assist in supporting the operationalisation of the 5th principle of the AI Ethics Framework - Reliability and safety - in the context of healthcare safety and quality.\\nAs part of its work plan, the Commission is developing resources to assist health services to evaluate and assess AI before the widespread uptake of these technologies. The resources aim to enable the safe implementation of AI into clinical practice and drive measurable improvements in the quality of patient care and outcomes.\\nThe Department recognises the potential need for additional regulatory and governance responses to ensure appropriate safeguards are in place and suggests accrediting the overarching governance processes of vendors developing AI technology, along with their device and software offerings, to instil trust and confidence in AI applications.\\nThe use of software and AI that performs a medical purpose, can be enhanced through further education of relevant health professional colleges and boards, higher education systems and training. Broader benefits could be gained by providing more accessible consumer and other stakeholder education as part of the government response to AI practices. These communication activities should include ensuring those products that are used for medical purposes have relevant TGA approval and that consumers understand the implication of AI and use of their personal information. There are many new stakeholders entering the market who are unaware of existing regulatory obligations and who do not fully understand their ongoing responsibilities.\\nThe TGA partners with ANDHealth to deliver webinars and education initiatives targeted to new entrants including those seeking to commercialise their product.\\nWithin the Australian Public Service (APS), the Government could consider performing an audit of current automated processes that use AI in order to promote  transparency and ensure AI is being used safely and responsibly within the APS.\\nTo effectively manage AI-related procurements, upskilling of government officials will be required and close monitoring of procurements will be essential to uphold compliance standards and mitigate potential risks associated with AI implementation.\\n5|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nEngagement of the health and aged care sector will be crucial to ensure equitable access to\\ntraining and education related to AI technology, to facilitate wider acceptance and\\nunderstanding.\\n4 Do you have suggestions on Whilst there is considerable activity related to AI occurring across government, more • As indicated in the paper, there are a\\ncoordination of AI consideration is required to understand how AI will apply in the health and aged care contexts as range of existing regulatory\\ngovernance across there are unique ethical, legal, and regulatory challenges that must be addressed. Existing frameworks that are relevant to AI\\ngovernment? regulatory frameworks and legislation are not sufficiently developed for the full utilisation of AI governance. However, there would be\\nPlease outline the goals that and are likely to require significant reform. This is particularly so in relationship to risks to value in coordination and information\\nany coordination human health and around privacy and trust in the release of sensitive health data for use in the sharing in response to related issues.\\nmechanisms could achieve development of AI tools. For example, privacy, copyright and\\nand how they could To achieve a cohesive approach, coordination mechanisms need to establish consistency and online safety issues associated with the\\ninfluence the development coherence in AI policies and regulations across government departments and agencies. The data used to train models will likely\\nand uptake of AI in Australia. coordination of AI governance also facilitates the effective sharing of knowledge and resources have similar intelligence and inquiry\\nand encourages inter-agency cooperation. Government agencies could leverage diverse needs and would benefit from relevant\\nexpertise and experiences to address challenges and capitalise on AI's opportunities. This instrumentalities having clear and\\nknowledge-sharing approach nurtures innovation, accelerates AI adoption, and ensures that the strong channels for information sharing\\ntechnology aligns with Australia's unique needs. Common principles and guidelines will minimise and referral of issues.\\npotential inconsistencies and create an overall strategy for AI adoption in Australia.\\nSector-specific governance of AI in healthcare is essential given the unique risks, challenges and\\nopportunities posed by AI in healthcare. The Australian Alliance for Artificial Intelligence in\\nHealthcare’s Roadmap for Artificial Intelligence in Healthcare for Australia could help inform\\nAustralia’s approach to managing the opportunities and risks that AI brings. Building public trust\\nand confidence in AI technologies will be important to ensure the uptake of AI solutions.\\nStreamlining procurement processes related to AI technologies is also another crucial outcome\\nof coordination mechanisms. A cohesive framework could guide government officials in AI-\\nrelated procurements, ensuring compliance with ethical standards and mitigating potential risks.\\nAs a result, the smooth implementation of AI applications becomes feasible, driving effective and\\nefficient use of the technology across government operations. Coordination mechanisms open\\navenues for cross-sector collaborations, bringing together government, academia, industry, and\\nother stakeholders. This inclusive approach fosters strategic partnerships and joint initiatives,\\nleading to transformative research and the development of AI solutions tailored to diverse\\nsocietal challenges.\\n6|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nDISR may wish to consider options for governance that supports ongoing inter and intra-\\ngovernmental engagement on matters such as AI that are not industry specific – such as privacy\\nor cybersecurity. The Department and the TGA already works with other governance bodies such\\nas Office of the Australian Information Commissioner (OAIC) and the Australian Cyber Security\\nCentre respectively, to ensure a cohesive approach to regulation.\\nThere is an opportunity for the Commonwealth to co-design with jurisdictions a national AI\\nethical and governance framework, in consultation with industry experts and the public. The\\nnew national framework would refresh the existing Commonwealth AI Ethics Framework and\\ncould be expanded to cover more specific guidance for key sectors in the economy where AI is\\nalready having, and will have, a significant impact, including healthcare. Such a framework could\\nform the basis to support for future self-regulation or government legislation.\\nRESPONSES SUITABLE FOR\\nAUSTRALIA\\n5 Are there any governance The Department is actively involved in ongoing surveillance of the international landscape with • DoHAC is actively involved in ongoing\\nmeasures being taken or the view to identifying new and emerging governance measures, particularly in the EU, UK, surveillance of the international\\nconsidered by other Canada, and the USA. There is a need to understand the contextual differences between what is landscape with the view to identifying\\ncountries (including any not acceptable in these counties versus in Australia and learn from their experiences and evaluation new and emerging governance\\ndiscussed in this paper) that of new initiatives. measures, in particular in the EU, UK,\\nare relevant, adaptable, and The TGA maintains a close relationship with other comparable regulators to ensure Canada and the USA.\\ndesirable for Australia? harmonisation of approaches. New requirements for software as a medical device (including AI) There is a need to understand the\\nare emerging in different jurisdictions including Europe, Canada, UK and the USA. The European contextual differences between what is\\nMedical Device Regulations (EU MDR 2017/745) has also introduced new requirements, acceptable in these counties versus in\\nincluding classifications specific for software as a medical device as part of a risk-based Australia and learn from their\\napproach. The European rule is consistent with the IMDRF recommendations. Since 2013, the experiences and evaluation of new\\nIMDRF (covering 10 regulators covering all major markets globally) has had in place, a dedicated initiatives. Consultation with the health\\nworking group reviewing Software as a Medical Device (SaMD), and AI – and publishes technical and aged care sectors will be essential\\ndocuments. The IMDRF undertakes global public consultation on all its work and participates in to ensure that governance measures\\nInternational Standards Organisation (ISO) standards, including those relating to SaMD. are fit for purpose.\\nA governance measure that is not discussed in the consultation paper that might help inform\\nAustralia’s approach to AI is the United States' proposed Algorithmic Accountability Act 2022.\\nThis legislation aims to establish a regulatory framework for assessing and mitigating bias and\\n7|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\ndiscrimination in AI systems used by large entities. It contains principles that might help Australia\\naddress algorithmic bias and promote fairness in AI applications.\\nConsultation with the health and aged care sectors will be essential to ensure that governance\\nmeasures are fit for purpose.\\nTARGET AREAS\\n6 Should different approaches No. There should be no differences between use in private or public sector for software and AI • In broad terms, the approaches to\\napply to public and private used for medical purposes. public and private sector use of AI\\nsector use of AI Under existing arrangements, the private sector cannot obtain health data for AI research in should be similar in that the\\ntechnologies? preparation for commercial purposes. The private sector can however with universities and they overarching principles should be similar\\nIf so, how should the can justify public interest considerations. and the risks posed by each sector’s\\napproaches differ? use are similar. However, the\\nGovernment public health data is largely administered (ie. without consent to gather data for regulatory framework should be alive\\nMedicare and PBS) and the use of this data in potential AI applications raises additional ethical to the different kinds of conflicts of\\nconsiderations that warrant thorough review and evaluation. Despite the potential risks, the use interest each sector may have in using\\nof sensitive personal information in the health and aged care sectors can provide a significant AI technologies. In the case of the\\npublic benefit by improving policy and service delivery. Balancing these benefits against the private sector, the typical concern is\\ninherent risks becomes crucial in contemplating specific regulations for AI activities in this that the profit motivation will lead to\\ndomain, ensuring that they do not excessively burden existing frameworks and hinder genuinely misuse of these technologies and\\npublic-interested activities. regulatory arrangements are crafted\\nThere are already sophisticated frameworks applied to the public sector that aim to ensure its accordingly.\\nfocus on the public interest (including reconciling conflicts between ends and means), for • In the case of the public sector, the\\nexample, parliamentary oversight and legislation, audit and other investigating bodies, FOI and concern is that there may sometimes\\n transparency regimes, and public sector codes of conduct. As a result, there appear to be be a conflict between ends and means,\\ndifferences in the uses broadly considered acceptable by the public sector, including allowing for that is:\\nthe public sector to engage in some higher risk applications of these technologies that would not o on the one hand, a public\\nbe generally considered acceptable by the private sector. body’s objectives (ends), which\\nshould be, by definition, in the\\nSensitive personal information can be used for the purposes of improving policy and service\\npublic interest, and\\ndelivery in the health and aged care sectors, which delivers a public benefit that can offset the\\no on the other hand, the public\\nrisks inherent in using such information. The key challenge in considering any specific regulation\\ninterest in the ways (means) in\\nof AI activities in this context, is that they do not create an excessive cumulative regulatory\\nwhich public bodies conduct\\nthemselves being fair, honest,\\n8|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nburden atop existing frameworks such that it is difficult to pursue genuinely public interested ethical and in line with natural\\nactivities. justice principles.\\nMaintaining consistent ethical principles across all areas where AI technologies are deployed is\\ncritical particularly in healthcare, given the frequent transitions of care between public and\\nprivate health services, as well as between care settings. These transitions involve the transfer of\\nsensitive health information, demanding careful consideration of the interoperability of AI\\ntechnologies to facilitate these transfers securely and efficiently.\\n7 How can the Australian Australians have high expectations of the Department in the handling of public information and • Provide funding for grants through the\\nGovernment further support building public trust will be critical. This requires careful consideration of data sharing practices Australian Government’s $20 billion\\nresponsible AI practices in its to determine what data can be shared, with whom, and under what circumstances. AI and Medical Research Future Fund (MRFF)\\nown agencies? ADM's reliability is heavily dependent on the quality and fairness of the data on which it is Applied Artificial Intelligence Research\\ntrained, and health and aged care data often exhibits strong gender and cultural biases, in Health.\\nnecessitating substantial work to improve data quality and comprehensiveness before it can be • AMA reports there is currently no\\nutilised in real-world applications. Efforts are underway to develop standard authorisation national framework for an AI ready\\nprovisions that facilitate greater data sharing and access. While synthetic data is often suggested health workforce. Use AI requires\\nas a remedy for privacy concerns, it is essential to recognise that its creation is resource- retraining of the workforce, retooling\\nintensive and may still require some real data. If The Department legislation requires reform, the health services and transforming\\nagency is prepared to contribute to this process. workflows. The health systems is\\nalready resource constrained and such\\nWith the speed of emerging technology, it will be critical for government to ensure that adoption changes will not happen without\\nof any new technology will not compromise public safety. Government can continue to support strategic investment (AMA Journal 13\\nresponsible AI practices in its own agencies through establishment of guidelines for best practice June 23).\\nand communication across portfolios to ensure a common understanding and implementation of\\npriorities, rules, best practice, and investment where required.\\nWhen evaluating AI model performance, considerations for vulnerable groups and clinicians are\\nvital to ensure fairness and equity in the outcomes. To continuously improve the AI systems'\\nimpact and performance on the health workforce, regular evaluation and monitoring are\\nessential. This includes assessing the effectiveness of AI applications, identifying, and addressing\\nbiases or unintended consequences, and adapting regulatory frameworks as needed.\\nTo support health professionals in making ethical decisions and ensuring patient-centred care\\nwhen using AI systems, the provision of ethical-decision making support tools and guidelines is\\nimperative. These tools will aid in navigating the complex ethical considerations that arise in the\\napplication of AI in healthcare. Government agencies should consider the recommendations\\n9|P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nprovided in the CSIRO Data 61 paper. This approach will foster the establishment of ethical\\nguidelines and practices for the responsible use of AI in the health sector.\\nCollaborative efforts involving health care professions, AI experts, policymakers, and regulatory\\nbodies are essential to develop comprehensive AI governance frameworks that align with the\\nspecific needs and challenges of the health sector. To achieve this, The Department suggests\\nprioritising investment in training and education to raise awareness of the use of AI and mitigate\\npotential risks of harm to humans.\\nTo optimise AI implementation, it is crucial to separate regulation from policy development,\\nproject management, and service delivery, as the utilisation of AI in these different areas varies\\nsignificantly.\\nIn addition to the recently published Interim guidance for agencies on government use of\\ngenerative AI platforms, consideration could be given to the following activities:\\n• Invest in training and education programs. For instance, further to priority 5 in the Australian\\nAlliance for Artificial Intelligence in Healthcare’s Roadmap for Artificial Intelligence in\\nHealthcare for Australia, the training workforce for the use of AI should also include policy\\nmakers.\\n• Establish an AI ethics review board or committee to oversee AI projects within government\\nagencies.\\n• Regularly assess and audit AI systems used by government agencies to identify and mitigate\\nany bias, discrimination, or unintended consequences.\\n• Collaborate with international organisations and governments to align responsible AI\\npractices and standards and foster knowledge-sharing.\\n• Establish mechanisms for reporting and addressing concerns or complaints related to AI\\nsystems used by government agencies.\\nMandatory reporting of AI use and activities (where relevant) through each Commonwealth\\nentity’s corporate plan to enable monitoring, oversight, compliance, and insights into evolving AI\\ntechnologies.\\n8 In what circumstances are Generic solutions and approach to AI are not suitable for AI used for medical purposes (ie: in • There will be little scope for generic\\ngeneric solutions to the risks medical devices). solutions to risks of AI in healthcare\\nof AI most valuable? settings. It will be important to have\\n10 | P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nAnd in what circumstances In healthcare alone there are a range of different settings including but not limited to public and technological and human based\\nare technology-specific private hospitals, primary care, aged care, acute care and National Disability Insurance Scheme. solutions that maximise the benefits\\nsolutions better? Please while reducing the harm.\\nFurther information about how risk based classification rules for software as a medical device\\nprovide some examples. apply is in Classification of active medical devices (including software-based medical devices).\\na) where and when  transparency will be most critical and valuable to  mitigate potential AI risks\\n9 Given the importance of •  Transparency could be partly\\n transparency across the AI and to improve  public trust and confidence in AI? addressed through regulation and\\nlifecycle, please share your  Transparency in the delivery of health care is essential. The Australian Charter of Healthcare obtaining consent from people for the\\nthoughts on: Rights states consumers should be given clear information about their condition, the possible use of AI technology in their diagnosis\\na. where and when benefits and risks of different tests and treatments, so they can give their informed consent. and treatment.\\n transparency will be most Consumers also need to be advised if their data will be used for any future purpose. To support •  Transparency is important to ensure\\ncritical and valuable to health professionals, it’s important for patients/consumers to have  transparency regarding the people are aware of the presence and\\n mitigate potential AI risks use of AI in their clinical/treatment decisions. function of AI in the products they are\\nand to improve public trust buying and the risks this may carry.\\n Transparency and the ability to explain the recommendations or outputs of AI and ADM systems\\nand confidence in AI? Both the general public and clinicians\\naffects the level of trustworthiness people have in the use of AI technologies.  Transparency and\\nneed to be equipped to understand\\nb. mandating  transparency explainability are at times used interchangeably, but it is useful to separate them out, and\\nthese risks.\\nrequirements across the discuss their differences. The Best Practice AI Regulation Toolkit notes that explainability is a\\n• Community awareness and education\\nprivate and public sectors, requirement that goes one step beyond mere  transparency. It seeks to explain how AI and ADM\\nis important and CALD communities\\nincluding how these has been applied and why a particular outcome has occurred.\\nwill need advice on AI in acceptable\\nrequirements could be For medical purpose devices that contain AI,  transparency is critical in: languages.\\nimplemented. • autonomous use where decisions are made primarily based on the AI output • Mental Health settings and guidance\\nwithout any other contextual information to verify accuracy will also need special attention due to\\n• adjunctive use where other contextual information such as patient symptoms, sensitive nature of mental health issues\\nphysical examination, lab testing or imaging are considered together with the AI and potential impacts.\\noutput.\\nb)  mandating transparency requirements across the private and public sectors, including how\\nthese requirements could be implemented.\\nThere should be  transparency consistent across public and private sectors and places the\\nconsumers of AI products at the centre of any policy or mandate.\\nIn the context of medical devices, manufacturers of a programmed or programmable medical\\ndevice including those that incorporate the use of AI, must be able to demonstrate compliance\\n11 | P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nwith the essential principles for safety, quality and performance. This is a legislative requirement\\nset out under the Therapeutic Goods (Medical Devices) Regulations 2002.\\nThe essential principles include specific requirements to ensure that medical device software is\\ndesigned and produced in a way that ensures the safety, performance, reliability, accuracy,\\nprecision, usability, privacy, security, and repeatability is appropriate for the intended use of the\\ndevice, and that suitable information on the labelling and instructions for use is provided to the\\nuser. Where AI is part of a consumer’s care it should be declared by health service organisations.\\nConsideration should be given to regulating the use of statements, similar to privacy statements.\\nThese statements could advise the consumer what part of their care is supported by AI, what\\ntype of AI is used – static or dynamic and what safeguards are in place to ensure user safety. It is\\nimportant that consumers are given information about the source of decisions made in their\\ncare. If AI is used in pathways of decision-making, it is important that the relevance of that\\ndecision making to the outcome of care is made transparent.\\nAIHW engages with the community regarding their trust or otherwise in complex work, such as\\ndata integration that necessitates use, at least initially, of identifiable data. A key feature of the\\nresponse is dependent on the uses to which those techniques are being applied. It is possible\\nthat the risks of using AI technologies would be approached similarly by the community and\\ntherefore  transparency and a clear articulation of the benefits of such technologies will be\\ncrucial to maintaining community trust. This would include, for example, clear explanations of:\\n• Why such technologies are necessary to the application in question and what specific public\\nbenefits they deliver over other approaches.\\n• How the technologies have been designed, developed, deployed, monitored, and\\nmaintained.\\n• What human oversight and checking there is of the results of the use of these technologies.\\nMechanisms are needed to encourage AI vendors to share or publish their research on how their\\ntools address bias. Regular reviews of algorithms are required to ensure biases don’t increase\\nover time (which they will if left unchecked). How often reviews of algorithms are required\\nshould be determined by the risks related to the application.\\na) The Department does not support blanket bans on applications or technologies without\\n10 Do you have suggestions for: • Rigorous testing by clinicians is\\nreview of potential benefits and risks against existing, or new, regulatory frameworks. The\\na. Whether any high-risk AI required where AI systems are allowed\\nDepartment continues to support risk-based approach to any regulatory frameworks.\\napplications or technologies to impact clinical decisions.\\n12 | P a g e\\nDepartment of Health and Aged Care\\nSafe and Responsible AI in Australia Submission\\nshould be banned Banning AI may be required for ethical reasons in some contexts, but it is more likely to be\\ncompletely? about upholding existing content areas bans. In a healthcare setting, it is not appropriate for\\nb. Criteria or requirements AI to both predict and respond to a health situation without human intervention.\\nto identify AI applications or Treatments recommended by AI may not consider the holistic needs of patients, such as\\ntechnologies that should be their values or preferences. For example, an AI algorithm may recommend healthcare based\\nbanned, and in which on what will lengthen a patient’s life expectancy, not taking into an account their preference\\ncontexts? for at home care or an ethical objection to certain treatments.\\nRegulation is required for high-risk AI technology applications in healthcare delivery.\\nConsideration could be given to the European Union's proposed AI Act, which lists high-risk AI\\nsystems including those relating to healthcare i.e. the management and operation of critical\\ninfrastructure, software for managing public healthcare services and electronic health records\\nb) Criteria or requirements to identify AI applications or technologies that should be banned,\\nand in which contexts\\nAI systems deemed to be high-risk should be inspected if they are going to be deployed and the\\ncreators of the system should have to show that it was trained on unbiased datasets in a\\ntraceable way and with human oversight.\\nRegulatory frameworks and risk assessments of new AI algorithms in healthcare are required,\\nparticularly for those that have the possibility of doing harm to human health. These\\nassessments could utilise many of the elements of existing health technology assessment\\nmechanisms such as Pharmaceutical Benefits Advisory Committee and Medical Services Advisory\\nCommittee.\\nIn a healthcare setting, we recommend considering patient safety and privacy requirements\\nwhen identifying whether any high-risk AI applications should be banned completely. AI\\nregulation is an area where the precautionary principle should be applied where potential harm\\nto individuals or society is present, and the likelihood of such harm to materialise even where\\nthere is a paucity of evidence.\\n11 What initiatives or There is a growing demand for appropriate policies in relation to the use of AI in government, to • See response to Question \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hPDfhc_TNzl9",
        "outputId": "40aa38fb-7ea1-4d3a-b124-85ddae16b4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              filename  \\\n",
              "0                        Australian Federal Police.txt   \n",
              "1    Office of the Australian Information Commissio...   \n",
              "2                         Law Council of Australia.txt   \n",
              "3                Digital Industry Group Inc (DIGI).txt   \n",
              "4               Department of Health and Aged Care.txt   \n",
              "..                                                 ...   \n",
              "242                                            OOP.txt   \n",
              "243                                     Complexico.txt   \n",
              "244                                Lilin Australia.txt   \n",
              "245                                      IdeaSpies.txt   \n",
              "246                           Codemaster Institute.txt   \n",
              "\n",
              "                                               content  \\\n",
              "0    21 August 2023\\nAustralian Federal\\nPolice sub...   \n",
              "1    Department of Industry, Science and Resources\\...   \n",
              "2    Safe and responsible AI in\\nAustralia\\nDepartm...   \n",
              "3    Department of Industry, Science and Resources,...   \n",
              "4    Department of Health and Aged Care\\nSafe and R...   \n",
              "..                                                 ...   \n",
              "242                                sine-conic fractals   \n",
              "243  Supporting responsible AI: Complexico's submis...   \n",
              "244  No! The definitions do not incorporate devices...   \n",
              "245  IdeaSpies is an open innovation platform shari...   \n",
              "246                                                yes   \n",
              "\n",
              "                                     transparency_text  \n",
              "0    transparency, accountability, fairness, privac...  \n",
              "1    public trust and confidence in AI.4 The OAIC h...  \n",
              "2    transparency.....................................  \n",
              "3    transparencyreport.google.com/youtube-policy/r...  \n",
              "4    transparency of AI in the delivery of health c...  \n",
              "..                                                 ...  \n",
              "242                                                     \n",
              "243  transparency, accountability, and trust in the...  \n",
              "244                                                     \n",
              "245                                                     \n",
              "246                                                     \n",
              "\n",
              "[247 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92c8cf04-7e19-4787-8225-b7bfca760599\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "      <th>transparency_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Australian Federal Police.txt</td>\n",
              "      <td>21 August 2023\\nAustralian Federal\\nPolice sub...</td>\n",
              "      <td>transparency, accountability, fairness, privac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Office of the Australian Information Commissio...</td>\n",
              "      <td>Department of Industry, Science and Resources\\...</td>\n",
              "      <td>public trust and confidence in AI.4 The OAIC h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Law Council of Australia.txt</td>\n",
              "      <td>Safe and responsible AI in\\nAustralia\\nDepartm...</td>\n",
              "      <td>transparency.....................................</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Digital Industry Group Inc (DIGI).txt</td>\n",
              "      <td>Department of Industry, Science and Resources,...</td>\n",
              "      <td>transparencyreport.google.com/youtube-policy/r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Department of Health and Aged Care.txt</td>\n",
              "      <td>Department of Health and Aged Care\\nSafe and R...</td>\n",
              "      <td>transparency of AI in the delivery of health c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>OOP.txt</td>\n",
              "      <td>sine-conic fractals</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>Complexico.txt</td>\n",
              "      <td>Supporting responsible AI: Complexico's submis...</td>\n",
              "      <td>transparency, accountability, and trust in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>Lilin Australia.txt</td>\n",
              "      <td>No! The definitions do not incorporate devices...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>IdeaSpies.txt</td>\n",
              "      <td>IdeaSpies is an open innovation platform shari...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>Codemaster Institute.txt</td>\n",
              "      <td>yes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>247 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92c8cf04-7e19-4787-8225-b7bfca760599')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-92c8cf04-7e19-4787-8225-b7bfca760599 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-92c8cf04-7e19-4787-8225-b7bfca760599');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f1438c9f-5701-4e93-a80b-2b2cf1d8c841\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1438c9f-5701-4e93-a80b-2b2cf1d8c841')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f1438c9f-5701-4e93-a80b-2b2cf1d8c841 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_00db2329-ec65-4fd1-b78c-e11ea0df8b0b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_files')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_00db2329-ec65-4fd1-b78c-e11ea0df8b0b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_files');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_files",
              "summary": "{\n  \"name\": \"df_files\",\n  \"rows\": 247,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 247,\n        \"samples\": [\n          \"Digital Media Research Centre, QUT.txt\",\n          \"Commonwealth Bank of Australia.txt\",\n          \"ANZSA.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 247,\n        \"samples\": [\n          \"Discussion Paper Submission\\nSafe and Responsible AI in Australia | 4 August 2023\\nRhyle Simcock, Nicholas Godfrey, A/Professor Anna Huggins, Professor Mark Burdon\\nSchool of Law/Digital Media Research Centre, Queensland University of Technology*\\nOverview\\nWe thank the Department of Industry, Science and Resources for the opportunity to make a submission in response to the Safe and Responsible AI in Australia discussion paper. We recognise the range of potential advantages that the adoption of AI can bring to both the public and private sectors. However, appropriate governance mechanisms are critical to ensure the responsible development of AI and to mitigate the accompanying risks.\\nAccordingly, Australia\\u2019s existing regulatory frameworks will need to evolve to accommodate the novel challenges posed by AI, and to ensure that adequate mechanisms are in place to address potential harms associated with its use.\\nThis submission draws on our combined expertise as researchers in privacy, public law and digital technologies to outline a number of regulatory reform recommendations for safe and responsible AI in Australia. Our main recommendations, related to questions 1, 2, 6, 7 & 9 of the discussion paper, are that the Australian Government should:\\n\\u2022 adopt different approaches to the regulation of AI in the public and private sectors.\\n\\u2022 amend legislation to safeguard the availability of judicial review for fully or partially\\nautomated decisions.\\n\\u2022 better understand the role of legal code in the digital compliance structures that will\\npower AI/ML developments in the public and private sectors.\\n\\u2022 introduce a statutory requirement for human oversight and review for public sector\\nautomated decision-making systems with serious consequences for individuals.\\n\\u2022 consider approaches to harmonising data quality standards across all levels of\\ngovernment.\\n1\\n\\u2022 consider the merits of design methodologies that promote transparency at all stages\\nof AI development.\\n\\u2022 reform federal and state freedom of information legislation, as well as government\\nprocurement practices, to facilitate open government ideals.\\nQ1. Do you agree with the definitions in this discussion paper? If not, what definitions do you prefer and why?\\nWe do not contribute specifically to the discussion on core definitions except to add that the adoption of language incorporated internationally through the ISO standard appears to be a sensible way forward and should assist to align Australia with developments in other jurisdictions. Given the increasing standardisation of core concepts through legislation with extra-territorial reach, such as the EU\\u2019s General Data Protection\\nRegulation (\\u2018GDPR\\u2019) framework, it would seem beneficial to adopt generally accepted definitions, rather than unique Australian constructions. However, it is important to note that the discussion paper solely focuses on definitions involving core technical processes of AI/machine learning (ML) without consideration of how different types of data/information could be used in those processes. Most notable is the absence of any definitional considerations of personal or sensitive information which is obviously relevant to the application of AI/ML systems. The current Attorney General\\u2019s Department\\nReview of the Privacy Act (\\u2018AG Review\\u2019) is examining whether the definition of personal and sensitive information should be more aligned with the conceptual basis of the GDPR.\\nIt seems non-controversial, but nonetheless important, to consider core definitions for the range of information that will power AI/ML systems and ensure there is consistency across existing and future legal regimes.\\nQ2. What potential risks from AI are not covered by Australia\\u2019s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?\\n2\\nAs noted in our response above, the discussion paper focuses squarely on the core conceptual processes of AI/ML and the risks arising from generated outputs. The paper tacitly acknowledges that the advent of new AI/ML will require major restructuring of data sharing and availability practices. Current processes are governed by a range of different legal frameworks as outlined in the paper. However, the paper does not engage deeply with the major information risks that could flow from widespread adoption of AI/ML processes and the concomitant requirements of industrialised, or government-wide, data sharing requirements. A major concern is information privacy and how the Australian\\nPrivacy Principles could apply to AI/ML driven technological structures. The AG Review appears to be suggesting stronger alignment between the Privacy Act 1988 (Cth) and the GDPR.\\nWe are strongly supportive of this alignment and repeat calls to better understand how alignment with the GDPR, and the possibility of broader alignments with other EU frameworks, such as the AI Act, could operate in Australia given the absence of an ostensible foundation of fundamental rights that guarantee stronger legal protections for\\nAustralian citizens.1 It is clear that the Australian Government sees AI/ML as a core part of Australia\\u2019s future digital economy and the digital governance structures it requires.\\nThese are by necessity driven by the types of technological components highlighted in the discussion paper. However, while these technical changes are necessary, they should not detract from the deeper and more critical issue of how Commonwealth law should best protect the fundamental rights of Australian citizens.\\nApplications involving the use of AI/ML also need to consider how both public and private sector organisations comply with legal and regulatory obligations, particularly in real- time workflows that produce automated forms of decision-making. Since 2018, we have\\n1\\nMark Burdon and Tegan Cohen (2023) Submission to the Attorney General's Department Privacy Act\\nReview Report https://eprints.qut.edu.au/242023/.\\n3\\nproduced research that outlines the complex challenges arising from the conversion of natural language legal and regulatory obligations into machine executable code that is intended to be used for digital compliance purposes.2 Our research has shown that it is a complex and challenging task to produce machine executable legal code that is both functional from a business use perspective and aligns with legal expectations involving judicially approved processes of statutory interpretation.3 Digital compliance processes predicated on the application of legal code in automated decision-making (\\u2018ADM\\u2019) systems will underpin the safe and responsible development of AI/ML in a more ubiquitous sense. The discussion paper is largely silent on this essential issue but it is nonetheless important to consider especially in relation to the development of new legal and regulatory structures, whether they be regulatory or self-regulatory in nature. A broader conceptual framing is required that considers how the technical components outlined in the discussion paper operate in current and future structures that are governed increasingly by automated forms of output built on legal code. As such, it is important to understand how digital compliance processes are conducted across the different logics and perspectives of legal, regulatory and computational based disciplines and professions.4\\nQ6. Should different approaches apply to public and private sector use of AI technologies? If so, how should the approaches differ?\\n2\\nSee, eg, Anna Huggins, Mark Burdon, Alice Witt and Nicolas Suzor, \\u2018Digitising Legislation: Connecting\\nRegulatory Mind-Sets and Constitutional Values\\u2019 (2022) 14(2) Law, Innovation and Technology 325; Alice\\nWitt, Anna Huggins, Guido Governatori and Joshua Buckley, \\u2018Encoding Legislation: A Methodology for\\nEnhancing Technical Validation, Legal Alignment and Interdisciplinarity\\u2019 (2023) Artificial Intelligence and\\nLaw, https://doi.org/10.1007/s10506-023-09350-1; Anna Huggins et al, \\u2018Submission No 196 to the Select\\nSenate Committee on Financial Technology and Regulatory Technology\\u2019 (Issues Paper Submission, 2020, https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Financial_Technology_and_Regulatory\\n_Technology/FinancialRegulatoryTech/Submissions); Anna Huggins, \\u2018Addressing Disconnection: Automated\\nDecision-Making, Administrative Law and Regulatory Reform\\u2019 (2021) 44(3) University of New South Wales\\nLaw Journal 1048.\\n3\\nMark Burdon et al, \\u2018From Rules as Code to Mindset Strategies and Aligned Interpretive Approaches\\u2019 (2023)\\nJournal of Cross-Disciplinary Research into Computational Law (forthcoming).\\n4\\nHuggins et al, \\u2018Digitising Legislation\\u2019 (n 2 above); Burdon et al (n 3 above).\\n4\\nWe believe that fundamental differences between the public and private sector necessitate different regulatory approaches to the use of AI. While private sector entities typically prioritise profitability, innovation, efficiency and corporate secrecy, traditional public sector principles of good administration include transparency, accountability, rationality, fairness and consistency. Government agencies are not subject to market forces and consumer choice like private entities. Consumers have the power to choose to engage with a different business for the delivery goods and services. However, there is no such freedom of choice in the delivery of government services. The potential risks from the misuse of AI can be heightened in the public sector. There is a significant power imbalance between the state and its citizens, and government use of AI can significantly impact the rights, interests and expectations of individuals. This warrants a higher degree of scrutiny and oversight of the public sector use of AI.\\nAustralia\\u2019s public law frameworks provide important mechanisms for regulating public sector use of AI, including safeguarding executive accountability and protecting individual rights and interests. However, regulatory reform is needed to address gaps in the application of\\nAustralia\\u2019s existing public law frameworks to ADM systems, including to safeguard the contestability of automated government decisions and to ensure human involvement in certain high stakes government decisions.\\nSafeguarding the Contestability of Automated Government Decisions\\nThere is some uncertainty about whether automated government decisions are judicially reviewable. The majority in Pintarich v Deputy Commissioner of Taxation held that a\\n\\u2018decision\\u2019 under the Administrative Decisions (Judicial Review) Act 1977 (Cth) (\\u2018ADJR Act\\u2019) requires a mental process of deliberation.5 This casts doubt on the availability of judicial review under the ADJR Act because ADM, by its very nature, lacks the requisite human mental processes to satisfy this criterion.6 This creates an unacceptable risk that individuals\\n5\\nPintarich v Deputy Commissioner of Taxation (2018) 262 FCR 41.\\n6\\nSee Yee-Fui Ng and Maria O\\u2019Sullivan, \\u2018Deliberation and Automation: When Is a Decision a \\u201cDecision\\u201d?\\u2019\\n(2019) 26(1) Australian Journal of Administrative Law 21.\\n5\\nadversely affected by erroneous or unlawful ADM systems will have limited options for redress.7\\nWe suggest that reform to clarify this legal uncertainty should be a priority for the Australian\\nGovernment. One option is to amend the definition of a \\u2018decision\\u2019 in the relevant State and\\nCommonwealth ADJR Acts to include decisions that are wholly or partly automated.8\\nAnother approach is to modify specific legislation authorising ADM to clarify the availability of judicial review. For example, a deeming provision could be inserted into the respective legislation to confirm that any ADM decision is considered a decision of the authorised decision-maker. We suggest that the first option is preferable given it is inclusive and does not require modification to individual pieces of authorising legislation.9\\nStatutory Protections for Human Involvement in Government Decision-making\\nExisting public law frameworks presuppose that humans remain at the core of the decision- making process. Human oversight and intervention is important for identifying and addressing problems in administrative processes. This remains true for the use of automated systems by administrative agencies. Manual human reviews of the automated debt notices under the Centrelink Online Compliance system (commonly known as\\n\\u2018robodebt\\u2019), for example, would likely have ameliorated many of the problems that arose during its operation. However, there are currently no statutory protections in place that require humans to oversee and review automated outputs.\\nWe suggest that there should be legislative mechanisms in place that mandate human involvement for certain types of automated administrative processes, particularly decisions which have potentially serious consequences for individuals. This aligns with previous recommendations by the Australian Law Reform Commission and the Australian Human\\n7\\nHuggins, \\u2018Addressing Disconnection' (n 2 above) 1061\\u20131064.\\n8\\nYee-Fui Ng et al, \\u2018Revitalising Public Law in a Technological Era: Rights, Transparency and Administrative\\nJustice\\u2019 (2020) 43(3) University of New South Wales Law Journal 1041, 1066.\\n9\\nIbid.\\n6\\nRights Commission that suggest exploring \\u2018the degree of human involvement, if any, that should be required for particular types of decisions\\u2019.10 Some guidance can be sought from the GDPR. Article 22, in particular, prohibits solely automated decision-making that affects individual rights and interests by requiring \\u2018meaningful\\u2019 human involvement. 11 While there are exceptions, minimum protections are in place to ensure that affected individuals have a right to obtain human intervention, to express their views or to contest automated decisions.12\\nQ7. How can the Australian Government further support responsible AI practices in its own agencies?\\nHarmonised Data Quality Standards\\nThe Australian Government should consider approaches to harmonising data quality standards across government, including its collection, management and use. The public sector use of ADM will become increasingly dependent on the exchange of data between administrative agencies. But Australia lacks unified data quality standards across Federal,\\nState and Local Government agencies, including standard definitions, units of measurement and accuracy benchmarks.13 Administrative agencies therefore often have different approaches to how data is organised, characterised and structured.14 This kind of data\\n10\\nAustralian Law Reform Commission, The Future of Law Reform: A Suggested Program of Work 2020\\u201325\\n(Report, December 2019) 24; Australian Human Rights Commission, \\u2018Human Rights and Technology\\u2019 (Final\\nReport, 1 March 2021) 71.\\n11\\nRegulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the\\nProtection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of\\nSuch Data, and Repealing Directive 95/46/EC (\\u2018GDPR\\u2019) [2016] OJ L 119/1 arts 22(2)(a)\\u2013(c). See also Article\\n29 Data Protection Working Party, \\u2018Guidelines on Automated Individual Decision-Making and Profiling for the\\nPurposes of Regulation 2016/679\\u2019 (Guidelines No WP251rev.01, 6 February 2018) 20-21\\n.\\n12\\nGDPR (n 11 above) art 22(3).\\n13\\nData Availability and Use: Productivity Commission Inquiry Report (Australian Government, Productivity\\nCommission, March 2017) 159\\u2013164.\\n14\\nPublic sector data is often fragmented, meaning administrative agencies rarely have enough data to accurately model an outcome of interest in the absence of inter-agency data sharing: Fola Malomo and\\nVania Sena, \\u2018Data Intelligence for Local Government? Assessing the Benefits and Barriers to Use of Big\\nData in the Public Sector\\u2019 (2017) 9(1) Policy & Internet 7, 9\\u201310. See also Andrew Iliadis, \\u02bbThe Tower of Babel\\nProblem: Making Data Make Sense with Basic Formal Ontology\\u02bc (2019) 3(6) Online Information Review\\n1021.\\n7\\nfragmentation can have detrimental impacts on the quality of the data, and consequently, the validity of ADM outputs.\\nWe note that the Data Availability and Transparency Act 2022 (Cth) introduced a legislative scheme for sharing Australian Government data. However, the Act does not contain any stipulations regarding data quality. Only the Australian Privacy Principles, enacted through the Privacy Act 1988 (Cth), impose an obligation on all private and public sector entities to ensure the quality and accuracy of information they hold.15\\nOne potential solution is for the Office of the National Data Commissioner, as the national regulator for Australian Government data sharing, to provide advice and guidance on inter- agency data quality standards. This might include, for example, technical best practice for the collection, management and use of government data nationwide.\\nQ9. Given the importance of transparency across the AI lifecycle, please share your thoughts on:\\na. where and when transparency will be most critical and valuable to mitigate\\npotential AI risks and to improve public trust and confidence in AI.\\nb. mandating transparency requirements across the private and public\\nsectors, including how these requirements could be implemented.\\nTransparency by Design\\nWe recommend consideration of design methodologies that promote transparency at all stages of development. In particular, the Government should consider:\\na) mandating technical and organisational record-keeping across the AI lifecycle.\\nContemporary technical and interdisciplinary scholarship has developed a suite of\\nvaluable tools for recording and disclosing information at different stages of the AI\\n15\\nPrivacy Act 1988 (Cth), sch 2 s 10.\\n8\\nlifecycle. This includes data documentation and provenance methods,16 model\\nperformance cards,17 auditing processes and logging mechanisms.18 Organisational\\nrecord-keeping is equally as important. Documentation about impact assessments,\\nprocurement processes and broader organisational choices should also be publicly\\navailable for scrutiny.19\\nb) adopting inherently transparent and interpretable AI systems in high-stakes settings.\\nAI complexity is a significant impediment to meaningful transparency.20 Such\\ninscrutability is often a symptom of system design.21 The more sophisticated kinds of\\nmachine learning models are not always necessary to achieve organisational goals.\\nNor do more complex models necessarily lead to better predictive performance.22 It is\\noften possible to achieve good predictive performance with much simpler machine\\nlearning models.23 Accordingly, simpler and inherently transparent AI systems should\\nbe used in domains where automated outputs will have significant consequences for\\naffected individuals.\\nc) consider the merits of acceptance test driven development (\\u2018ATDD\\u2019) methodologies.\\nATTD approaches, such as the development of interdisciplinary \\u2018user stories\\u2019, can\\nclose the understanding gap between policy designers and developers of ADM\\nsystems, while also generating concise natural language documentation which can\\nconvey the intent of specific system operations. While ethical guidelines can be useful,\\n16\\nTimnit Gebru et al, \\u2018Datasheets for Datasets\\u2019 [2018] arXiv:1803.09010 [cs]\\n.\\n17\\nMargaret Mitchell et al, \\u2018Model Cards for Model Reporting\\u2019 (Conference Paper, Conference on Fairness,\\nAccountability, and Transparency, 2019) .\\n18\\nJoshua A Kroll, \\u2018Outlining Traceability: A Principle for Operationalizing Accountability in Computing\\nSystems\\u2019 (Conference Paper, Conference on Fairness, Accountability, and Transparency, 2021)\\n; Jatinder Singh, Jennifer Cobbe and Chris Norval,\\n\\u2018Decision Provenance: Harnessing Data Flow for Accountable Systems\\u2019 (2019) 7 IEEE Access 6562.\\n19\\nJennifer Cobbe, Michelle Seng Ah Lee and Jatinder Singh, \\u2018Reviewable Automated Decision-Making: A\\nFramework for Accountable Algorithmic Systems\\u2019 (Conference Paper, Conference on Fairness,\\nAccountability, and Transparency, 2021) .\\n20\\nJenna Burrell, \\u2018How the Machine \\u201cThinks\\u201d: Understanding Opacity in Machine Learning Algorithms\\u2019 3(1)\\nBig Data & Society 1, 4\\u20135.\\n21\\nJoshua A Kroll, \\u2018The Fallacy of Inscrutability\\u2019 (2018) 376(2133) Philosophical Transactions of the Royal\\nSociety of London 20180084.\\n22\\nCynthia Rudin, \\u2018Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use\\nInterpretable Models Instead\\u2019 (2019) 1(5) Nature Machine Intelligence 206.\\n23\\nIbid.\\n9\\nAI developers cannot always map principles to practical development.24\\na. Ethical User Stories25 have been proposed to increase alignment of ADM\\nsystems26 by outlining links between abstract ethical values and functional\\nrequirements.27 A preliminary study comparing the use of ECCOLA,28 a\\nmethodology for ethical user story construction, with standard user story\\napproaches, revealed that the former produced noticeably more \\u2018human-\\ncentric\\u2019 user stories, while standard user story methods generated significantly\\nmore \\u2018technology-centric\\u2019 user stories.29 The study further showed that, when\\ncompared to an independent user story evaluation model,30 the ECCOLA\\nmethod produced higher-scoring stories, indicating that even beyond ethical\\nconsiderations, there is value for developers and other stakeholders in\\nadoption of the method.31\\nb. Value sensitive design32 methodologies such as participatory design have\\nbeen increasingly explored to embed desired values into AI systems.33 These\\napproaches reduce the likelihood of marginalised groups being unfairly\\nimpacted.34 They have been used to shift the design of legal AI systems\\u2019 goals\\n24\\nVille Vakkuri et al, \\u2018ECCOLA \\u2014 A Method for Implementing Ethically Aligned AI Systems\\u2019 (2021) 182\\nJournal of Systems and Software 111067; Erika Halme et al, \\u2018How to Write Ethical User Stories? Impacts of the ECCOLA Method\\u2019 in Peggy Gregory et al (eds), Agile Processes in Software Engineering and Extreme\\nProgramming (Springer International Publishing, 2021) 37.\\n25\\nEthical user stories differ from standard user stories in that the former also considers non-functional user requirements: Erika Halme et al, \\u2018Ethical User Stories: Industrial Study\\u2019 in Joint Proceedings of REFSQ-2022\\nWorkshops, Doctoral Symposium, and Posters & Tools Track (CEUR-WS, 2022) 5.\\n26\\nIbid; Halme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 36.\\n27\\nQinghua Lu et al, \\u2018Towards a Roadmap on Software Engineering for Responsible AI\\u2019 in Proceedings of the\\n1st International Conference on AI Engineering: Software Engineering for AI (Association for Computing\\nMachinery, 2022) 101, 105 .\\n28\\nVakkuri et al (n 24 above).\\n29\\nHalme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 45-46, 49.\\n30\\nLuigi Buglione and Alain Abran, \\u2018Improving the User Story Agile Technique Using the INVEST Criteria\\u2019 in\\n2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th\\nInternational Conference on Software Process and Product Measurement (2013) 49.\\n31\\nHalme et al, \\u2018How to Write Ethical User Stories\\u2019 (n 24 above) 46-47, 49-50.\\n32\\nBatya Friedman, \\u2018Value-Sensitive Design\\u2019 (1996) 3(6) Interactions 16.\\n33\\nSteven Umbrello, \\u2018Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design\\nApproach\\u2019 (2019) 3(1) Big Data and Cognitive Computing 5.\\n34\\nSee, eg, Q Vera Liao and Michael Muller, \\u2018Enabling Value Sensitive AI Systems through Participatory\\nDesign Fictions\\u2019 (No arXiv:1912.07381, arXiv, 12 December 2019) .\\n10\\nand methods from computer scientists to legal experts, as well as decrease\\nthe knowledge gaps between such domain experts.35 Further, participatory\\ndesign has a history of being used in the development of expert systems to\\nreduce abstraction,36 thereby encouraging the development of decision-\\nmaking systems which can be understood with limited computational\\nproficiency. Participatory design can be used across various points of\\ndevelopment.37 Like ethical user story approaches, they generate artifacts\\nwhich are collaboratively constructed by computer scientists and domain\\nexperts alike.38 Accordingly, they can be used to audit and understand the\\ngoals and methods of ADM systems. In this way, the implementation of ethical\\nuser stories or participatory design methodologies generates transparency as\\na by-product, and therefore may be palatable to developers concerned about\\ninefficiencies and the costs of compliance with transparency mandates.\\nSafeguarding Transparency in the Government Use of AI\\nTransparency and explainability are critically important for all uses of AI, but they are especially crucial in the public sector where there is an expectation of higher standards of transparency and accountability. AI systems used in the public sector should be sufficiently transparent to permit public scrutiny and facilitate contestation where necessary. In line with open government and transparency ideals, we recommend introducing the following regulatory reform solutions aimed at tackling AI opacity barriers in the public sector:\\na) Reforming federal and state freedom of information legislation to ensure that\\nsufficient information about Government AI can be obtained by individual citizens,\\nmedia and civil society groups. Government refusal of some freedom of information\\n35\\nFernando Delgado, Solon Barocas and Karen Levy, \\u2018An Uncommon Task: Participatory Design in Legal\\nAI\\u2019 (2022) 6(CSCW1) Proceedings of the ACM on Human-Computer Interaction 51:1-51:23, 51:2, 51:5.\\n36\\nIbid 51:5.\\n37\\nDouglas Zytko et al, \\u2018Participatory Design of AI Systems: Opportunities and Challenges Across Diverse\\nUsers, Relationships, and Application Domains\\u2019 in Extended Abstracts of the 2022 CHI Conference on\\nHuman Factors in Computing Systems (Association for Computing Machinery, 2022) 1, 2\\n (\\u2018Participatory Design of AI Systems\\u2019).\\n38\\nJeanette Blomberg, Lucy Suchman and Randall H Trigg, \\u2018Reflections on a Work-Oriented Design Project\\u2019\\n(1996) 11(3) Human\\u2013Computer Interaction 237.\\n11\\nrequests was identified as a significant impediment to meaningful scrutiny of the\\nCentrelink online compliance intervention scheme.39\\nb) Establishing public sector procurement standards that prioritise transparency over\\ncommercial secrecy and outline what specific information administrative agencies are\\nrequired to collect and disclose about AI systems in use.40 These standards should\\nalso be used to facilitate openness in freedom of information requests when AI-\\nsystems are developed by, or in conjunction with, private contractors.\\nc) Instituting a proactive disclosure regime containing a public register of Government\\nAI systems in use, as well as public disclosure of technical documentation sufficient\\nto facilitate external scrutiny and individual contestation. Ideally, government\\nagencies should be required to make the source code of AI publicly available. We\\nrecognise, however, that source code transparency alone may be insufficient for\\nachieving meaning accountability and transparency in AI systems,41 particularly for\\nadversely affected individuals.42 Therefore, in addition to source code disclosure,\\ntechnical and organisational documentation of coding decisions should also be made\\npublicly available.\\nd) Modifications to the ADJR Act that explicitly require the provision of a statement of\\nreasons for all decisions made by, or in reliance on, an ADM system. Reasons are\\nessential for the contestation of automated administrative decisions. Affected\\npersons should be able to understand the rationality behind a decision and be given\\nenough information to contest it where necessary. However, we note that there is\\nsignificant doubt about the ability for fully automated AI systems to produce \\u2018legally\\n39\\nSee Ashlynne McGhee, \\u2018Centrelink Debt Recovery Program: Department Rejects FOI Requests Relating to Plagued Scheme\\u2019, ABC News (online, 10 February 2017) .\\n40\\nCatherine Holmes, Royal Commission into the Robodebt Scheme (Report, 2023) 656\\u2013657.\\n41\\nJoshua A Kroll et al, \\u2018Accountable Algorithms\\u2019 (2016) 165(3) University of Pennsylvania Law Review 633,\\n647\\u2013650.\\n42\\nSee, eg, Mike Ananny and Kate Crawford, \\u2018Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability\\u2019 (2018) 20(3) New Media and Society 973; Deven R Desai and Joshua A Kroll, \\u2018Trust but Verify: A Guide to Algorithms and the Law\\u2019 (2017) 31(1) Harvard Journal of\\nLaw and Technology 1.\\n12\\ncompelling\\u2019 reasons for decisions.43 Technical explanations, including those\\nproduced by modern explainability tools, often fall short of providing the type of\\nreasons needed to facilitate understanding and contestation of an administrative\\ndecision. Reasons must go beyond the logic of how the decision was made and\\ninclude reasons as to why the decision was made.44 In circumstances where this is\\nnot possible, we suggest that ADM should not be used for government decision-\\nmaking purposes.\\n* About QUT\\u2019s Digital Media Research Centre\\nThe Digital Media Research Centre (\\u2018DMRC\\u2019) at the Queensland University of Technology is a leading research centre in digital humanities and social science research with a focus on digital communication, media, and the law. Our research programs investigate digital inclusion and participation, the digital transformation of media industries, the growing role of\\nAI and automation on digital societies and the role of social media in public communication.\\nScholars in the DMRC are undertaking important research on many of the concerns raised by the discussion paper. This includes ongoing work to investigate the development of regulatory regimes to address emerging risks for AI and ADM.\\n43\\nWill Bateman, \\u2018Algorithmic Decision-Making and Legality: Public Law Dimensions\\u2019 (2020) 94(1) Australian\\nLaw Journal 520, 527.\\n44\\nJennifer Cobbe, \\u2018Administrative Law and the Machines of Government: Judicial Review of Automated\\nPublic-Sector Decision-Making\\u2019 (2019) Legal Studies 1, 22.\\n13\",\n          \"\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\",\n          \"About us\\n1 This submission is made on behalf of the Australia New Zealand Screen Association (ANZSA).\\nThe ANZSA represents the film and television content and distribution industry in Australia and\\nNew Zealand. Its core mission is to advance the business and art of filmmaking, increasing its\\nenjoyment around the world and to support, protect and promote the safe and legal\\nconsumption of movie and TV content across all services. Members of ANZSA include: Village\\nRoadshow Limited; Motion Picture Association; Walt Disney Studios Motion Pictures; Netflix\\nInc.; Paramount Pictures; Sony Pictures Releasing International Corporation; Universal\\nInternational Films, Inc.; and Warner Bros. Pictures International, a division of Warner Bros.\\nEntertainment Inc., and Fetch TV.\\nGeneral comments\\n2 We thank the Government of Australia and the Department of Industry, Science and Resources\\nfor the opportunity to provide comments on the discussion paper titled Safe and responsible AI\\nin Australia (\\u201cthe discussion paper\\u201d). We welcome the Department\\u2019s forward-looking approach\\nin opening the consultation, and for the opportunity for interested parties like ANZSA and our\\nmembers to share our input. We encourage the Government to continue providing frequent and\\ntransparent opportunities for public input on the topic of artificial intelligence (\\u201cAI\\u201d)1 regulation,\\nso that interested stakeholders can continue providing feedback on this pressing and important\\nissue. We also urge the Government not to rush to regulate AI or to impose new and hasty\\nrules on the use of AI. Further development of regulation, if and when there is a demonstrated\\nneed to regulate, should be done in close consultation with all relevant stakeholders, including\\ninterested industry representatives.\\n3 We note the acknowledgement in the discussion paper that \\u201cthe range of contexts in which AI\\ncan be used, and for different purposes, may necessitate context-specific resources\\u201d, and that\\nthe consultation \\u201cdoes not seek to consolidate or replicate the development of existing general\\nor sector-specific regulations and governance initiatives across the Australian Government.\\u201d\\nAny broad new AI regulation with application across multiple sectors will also run the risk of\\noverlapping with existing regulation, causing regulatory uncertainty, which would create a\\nheavy compliance burden and disincentivize business investment.\\n4 Governments should consider carefully, and in consultation with relevant stakeholders and\\ninformed by an assessment of risk and potential for harm, the need for a regulatory framework\\nfor AI. If such a framework is found to be necessary, any obligations must be proportionate to\\nthe potential for harm. Any burdensome requirements will add another layer of compliance\\n1 We note that AI is a term used broadly but covers many technologies. Generative AI refers to a subset of artificial\\nintelligence that learns patterns from data and, when directed by a person through \\u201cprompting\\u201d, produces content based\\non those patterns. This contrasts with \\u201ctraditional AI\\u201d systems that are used to predict outcomes or generate insights.\\nburden on companies and could have major unintended consequences, including providing a\\nstrong disincentive to further investment for content creators. Broadly speaking, the creative\\nsector flourishes best in a context of light-touch regulation that encourages ease of doing\\nbusiness, both domestically and internationally.\\n5 In this regard, we support the Government\\u2019s consideration of a risk management approach that\\n\\u201ccaters to the context-specific risks of AI\\u201d and \\u201callows for less onerous obligations for lower risk\\nAI uses\\u201d. We note (in Box 4, page 32 of the discussion paper) that the Government considers\\nthe use of \\u201cAI-enabled recommendation engines to enable personalised online shopping\\nrecommendations based on users\\u2019 browsing history, preferences and interests\\u201d as a low-risk\\nuse case, with a \\u201climited, reversible or brief\\u201d impact that should allow for fewer or no obligations\\nfor its use.\\nAI can support human creativity\\n6 ANSZA member companies are already using or plan to use AI to support the creation and\\ndelivery of a wide range of works that bring benefits (both economic and cultural) to society.\\nRecent advances in AI technology, including the rise of generative AI tools like Midjourney and\\nDall-E, are leading to major and complex debates around the copyright implications of this\\ntechnology; and to intensifying pressure to regulate the use of AI, both in Australia and in other\\njurisdictions around the world. However, we note that notwithstanding these substantial\\ndevelopments, the AI field is still in a relatively nascent stage \\u2013 there are still further\\ndevelopments to come.\\n7 AI is an enabling tool that can complement aspects of filmmaking process, the audience\\nviewing experience, and fan engagement. We would note that the use of AI is not novel and\\nhas been employed as a tool in the production process, particularly in the context of special\\neffects. For example, \\u201ctraditional AI\\u201d has been used in a number of ways in production, such as\\nto predict resource usage, optimization of shooting schedules, and predicting complexity of\\nVFX shots. AI is also used in fairly routine post-production work like colour correction, detail\\nsharpening, de-blurring, or removing unwanted objects. Some are more involved, like aging\\nand de-aging an actor.\\n8 Across each of these examples, it is clear that AI use cases across the screen industry are\\ninherently low risk to consumers. They do not present the same level of harm as generating\\nmanipulated media for the purposes of misleading or deceiving a consumer. Instead, it is a\\nvaluable tool in the production process that is becoming increasingly important to present\\nvisually compelling experiences for audiences.\\n9 The rapid availability of generative AI has added additional layers of possibility as well as\\ncomplexity. Notably, we do not believe it will replace human creativity. Instead, ANZSA\\nmembers believe AI and generative AI will serve to free up humans from the most rote parts of\\ntheir work, allowing them to concentrate their limited time and effort on the most creative\\naspects.\\nDifferentiating the use of AI in curated VOD services versus user generated content\\nservices\\n10 Several ANZSA members operate, or are planning to operate, video-on-demand (VOD)\\nservices in Australia which offer large and diverse catalogues with human-curated and\\nprofessionally produced content. This human curation element is crucial; ANZSA\\u2019s members\\ncomply with existing regulation to ensure that viewers are appropriately informed about the\\ncontent they are about to watch, or which they allow their children to watch. These VOD\\nservices use various recommendation systems (some of which could be enhanced by AI) to\\nhelp viewers find content that most closely suits their interests. ANZSA submits that the use of\\nAI within this framework is inherently low risk.\\n11 This contrasts with user generated content services which do not curate their content offering\\nand as a result the risk of adverse outcomes by purely AI-driven recommendation processes is\\ngreater in this situation. ANZSA submits that regulation should recognise the differentiation\\nbetween VOD services and providers of user generated content in terms of content\\nresponsibility, consumer interest and creative-led freedom of expression, particularly with\\nrespect to public policy concerns about the potential use of AI in promoting misinformation and\\ndistributing harmful content via digital platforms.\\n12 The recommendation systems used in VOD services pose little or no risk of the type that\\nwarrant regulation. VOD services\\u2019 recommendation engines should therefore clearly be\\nconsidered very low-risk uses of AI, with few or no obligations imposed on their use.\\nAI and Copyright\\n13 ANZSA members, and the creative community more broadly, rely on strong and effective\\ncopyright legislation and policy to protect their production of and investment in creative content,\\nwhich is enjoyed around the world. Such copyright policy is of utmost importance to the creative\\ncommunity and requires considerable attention from the relevant experts, especially given the\\nhost of issues that AI has brought up in relation to intellectual property.\\n14 As the discussion paper notes, the Attorney-General\\u2019s Department (AGD) is already in the\\nprocess of organizing Ministerial Roundtables on copyright, including an upcoming one\\nscheduled for August 2023 on the implications of AI for copyright law. Given the sectoral\\nexpertise of the AGD, copyright policy should fall under its remit and the process coming out of\\nthe Ministerial Roundtables. ANZSA and its members are currently participating actively in the\\nMinisterial Roundtables.\\n15 This AI consultation led by the Department of Industry, Science and Resources should have a\\nbroader remit addressing other issues. Should a report be released following this consultation,\\nit should also note that copyright-related AI issues fall outside the Department\\u2019s purview and\\nwill be addressed by the AGD.\\n16 We thank the Department again for providing the opportunity to provide our comments on this\\nimportant topic. We are happy to meet to discuss these comments.\\nPaul Muller\\nCEO Australia New Zealand Screen Association\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transparency_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 191,\n        \"samples\": [\n          \"transparency and oversight of AI systems. Even if the underlying model remains proprietary, in an analogous fashion to proprietary software code, using open benchmarks we will have a way to evaluate any AI system for its alignment to and achievement of a given set of policy objectives.\\n5\\n\\\"People's Speech,\\\" MLCommons, accessed July 7, 2023, https://mlcommons.org/en/peoples-speech/.\\n6\\n\\\"Multilingual Spoken Words,\\\" MLCommons, accessed July 7, 2023, https://mlcommons.org/en/multilingual-spoken-words/.\\n7\\n\\u201cDynabench\\u201d, accessed July 7, 2023, https://dynabench.org/.\",\n          \"Transparency in Technologies Augmenting Consumer\\nDecision-Making\\u2019 (2023) 34 Loyola Consumer Law Review 558.\\n6Cf Australian Law Reform Commission, Review of the Legislative Framework for Corporations and Financial Services Regulation\\n.\\n3\\ncovered by Australia\\u2019s existing regulatory/legal regimes.7 However, the aim of effective regulation should be to reduce the likelihood harm occurring. Accordingly, we think that consideration should be given to mandating a greater role for technical standards, risk assessment frameworks and strong mechanisms for accountability and governance to reduce the likelihood of harm from AI to individuals, society, and the planet.8 Moreover, there is currently considerable uncertainty about how existing legal and regulatory regimes apply to AI, and, potentially, practical hurdles in establishing wrongdoing arising from the opacity of AI systems. Our preferred kinds of ex ante intervention may further reduce these concerns by aiding in the process of establishing wrongdoing and enforcing prohibition on the offending conduct.9\\nAI risks\\nMany AI risks are now well documented. It is nevertheless vital to consider these to ensure effective regulation for reducing risk and promoting beneficent outcomes. The risks of AI include:\\n\\u25cf Technical risks:\\no Inaccuracy: produced by drift, lack of robustness, inaccuracy, bias;\\no Information leakage: Some AI and ML models make decisions based on the past data used\\nto develop them thereby potentially disclosing information about this data to third parties\\nusing these models;\\no Adversarial manipulations: Input in AI and ML models comes from human prompts, other\\nsystems, or surroundings (e.g., videos, images, sounds). These inputs can be intercepted\\nand manipulated, causing AI and ML models to behave in an adversarial manner (e.g.,\\navoiding recognition of a STOP sign or generating a text with negative emotions). These\\nrisks have to be adequately addressed or at least highlighted;\\n\\u25cf Human rights risks: including a lack of equity and access, bias and discrimination, the erosion of\\nprivacy, the undermining of rule of law values;\\n\\u25cf Societal risks: misinformation and deepfakes disrupting democratic processes, civil society, and\\nmarkets;\\n\\u25cf Existential risks: arising from concerns about what it means to be human and how do we\\nunderstand human machine interactions.\\nOther human systems and behaviours also raise many of these risks of harm. Thus, it might be asked why the use of AI should be subject to special attention. We suggest that the character and magnitude of the risks from AI can be difficult to predict. This is so because AI systems may:\\n\\u25cf develop and come onto the market abruptly;\\n\\u25cf be built and deployed by a range of actors and states;\\n\\u25cf be put to unexpected and novel uses; and\\n7 But in the context of consumer protection law see the case for a prohibition on unfair commercial practices: Jeannie Paterson,\\nElise Bant, Nicholas Felstead and Eugene Twomey, \\u2018Beyond the unwritten law: The limits of statutory unconscionable conduct\\u2019\\n(2023) 17 Journal of Equity 1.\\n8 Jeannie Marie Paterson, Shanton Change, Marc Cheong, Chris Culnane, Suellette Dreyfus and Dana McKay, \\u2018The Hidden Harms of Targeted Advertising by Algorithm and Interventions from the Consumer Protection Toolkit\\u2019 9 International Journal on Consumer\\nLaw and Practice 1; Jeannie Marie Paterson, \\u2018Making robo-advisers careful? Duties of care in providing automated financial advice to consumers\\u2019 18 Law and Financial Markets Review.\\n9See Jeannie Paterson, \\u2018Misleading AI: Regulatory Strategies for Algorithmic  Transparency in Technologies Augmenting Consumer\\nDecision-Making\\u2019 (2023) 34 Loyola Consumer Law Review 558.\\n4\\n\\u25cf be opaque, meaning harms such as discrimination may take time to identify and may be\\nembedded in proxies or correlations rather than direct reliance on protected attributes.10\\nGeneral law and statute\\nThe use and outputs of AI systems will be subject to the law applying in the context in which the AI system is being used. For example, an AI service provided without reasonable care will be subject to the tort of negligence. AI bias may be contrary to anti-discrimination legislation or unconscionable under consumer law. Inaccurate or unlawful decisions produced using AI may breach administrative or corporations law, depending on the context.\\nHowever, and as already, noted it is desirable to undertake measures to reduce the likelihood of harms occurring or, in other words, embed accountability. It may also be desirable to introduce regulatory requirements that will assist regulators or individuals and businesses subject to harm by AI to understand where and how AI has been used, and to establish the cause of the harm, such as through requirements of  transparency, explanations and audits.\\nThese kinds of demands are key requirements of ethical or responsible AI.\\nPrinciples of AI ethics\\nAI principles are described as soft law because they are not formal law but nonetheless influence those subject to the law and the interpretation of the law that applies. Principles of AI ethics are the starting point for understanding the responsibilities of those developing, deploying, and using AI tools.\\nKey governance requirements in many formulations of AI principles are  transparency, explainability contestability and accountability.\\n Transparency\\n Transparency is a requirement to provide information about where and how AI is being used to\\ninform decision making, and the weightings or process influencing a decision or\\nrecommendation.11\\nIn some higher-risk instances it may also be necessary to provide access for public auditing of\\nmodel weights, training data, model outputs, and model code.\\nExplanations\\nStatements that provide clarity around how decisions or recommendations are reached.12\\nContestability\\nClear and easily navigated processes for contesting the outputs of AI that affect individual rights\\nor entitlements.13\\n10 McLoughney, Aidan James, Paterson, Jeannie Marie, Cheong, Marc, and Wirth, Anthony 2023. \\\"\\u2018Emerging proxies\\u2019 in information-rich machine learning: a threat to fairness?.\\\" 2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS) doi:10.1109/ethics57328.2023.10155045. Aidan James McLoughney, Jeannie Marie Paterson, Marc\\nCheong and Anthony Wirth, \\u2018\\u201cEmerging proxies\\u201d in information-rich machine learning: a threat to fairness?\\u2019 (Conference Paper,\\nIEEE International Symposium on Ethics in Engineering, Science and Technology (ETHICS), 18 May 2023),\\n11See Jeannie Paterson, \\u2018Misleading AI: Regulatory Strategies for Algorithmic  Transparency in Technologies Augmenting Consumer\\nDecision-Making\\u2019 (2023) 34 Loyola Consumer Law Review 558..\\n12 On state of the art studies into explanations see Tim Miller, \\u2018Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven\\nDecision Support using Evaluative AI\\u2019 (Conference Paper, FAccT \\u201923: Proceedings of the 2023 ACM Conference on Fairness,\\nAccountability and  Transparency, June 2023); Mor Vered, Tali Livni, Piers Douglas Lionel Howe, Tim Miller and Liz Sonenberg, \\u2018The\\nEffects of Explanations on Automation Bias\\u2019 (2023) 322(9) Artificial Intelligence.\\n13Henrietta Lyons, Senuri Wijenayake, Tim Miller and Eduardo Velloso, \\u2018What\\u2019s the appeal? Perceptions of Review Processes for\\nAlgorithmic Decisions\\u2019 (Conference Paper, 2022 CHI Conference on Human Factors in Computing Systems, April 2022); Henrietta\\n5\\nAccountability\\nAccountability requires systems and processes for overseeing and reducing the risks of any AI tool,\\nincluding for example:\\no Testing, reviewing, cleaning the data;\\no Auditing outcomes to look for patterns of disadvantage against protected attributes\\nand groups;\\no Inclusive design; and\\no Ongoing governance.\\nNotably, accountability means much more than having a \\u2018human in the loop\\u2019, which may do little\\nto address concerns about AI due to the impact of factors such as opacity and human automation\\nbias.14\\nMany of these principles are now laid out in technical standards, such as those developed by IEEE and\\nStandards Australia.\\nWe discuss these requirements further below in response to question 14.\\nMaking standards mandatory\\nWe note the principles of AI ethics are not solutions in and of themselves, but consider they should be part of the regulatory mix for ensuring responsible AI. However, to the extent these ex-ante principles are significant in reducing the risks of harm of AI products, we suggest that thought should be given ways of making firms and other entities in the AI lifecycle engage with them fully and impactfully.\\nWe envisage this might be done by requiring risk assessments for AI outputs and use cases, a strategy for responding to that risk through governance and accountability frameworks consistent with best practice principles, and, possibly, providing that compliance with technical standards frameworks is evidence of best practice (although not conclusive).\\nImportantly, we consider that any risk-based approach should avoid prescriptive rules (see below at 14).\\nPrescriptive rules for how firms respond to the demands of responsible AI are likely to prove unfit for purpose. They are likely to lag behind technological developments and prove a poor fit for the contribution of the different entities that might be part of the AI lifecycle.\\nThus, and subject to our comments on bans below under question 10, we prefer a principles-based approach to regulation. We note that a principles-based approach is not the same as self-regulation (which we do not support) or even a soft law approach (since although it may make use of soft law standards it contains legally binding principles).\\nUnder a principles based approach to regulation, legally binding principles would express expected standards of conduct and performance (e.g. risk assessments, governance/accountability, standards of safety). The detailed guidance as to what that conduct might require would be found in low level or soft guidance able easily to be changed in line with the market or developments in technology.\\nPrinciples based regulation allows greater responsiveness to future change. Any AI regulatory regime must be designed to be agile. The future direction of technological innovation is uncertain\\u2013whether the bulk of\\nAI use will occur on device, or in the cloud and further, whether it will be large entities providing most models or networks of individuals. Like the early days of the internet, the direction of development impacts\\nLyons, Eduardo Velloso and Tim Miller, \\u2018Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions\\u2019 (2021)\\n5(CSCW1) Proceedings of the ACM on Human-Computer Interaction 1.\\n14Mor Vered, Tali Livni, Piers Douglas Lionel Howe, Tim Miller and Liz Sonenberg, \\u2018The Effects of Explanations on Automation Bias\\u2019\\n(2023) 322(9) Artificial Intelligence.\\n6\\nthe effectiveness of various forms of regulation and a nimble approach is therefore required.\\nThe relevance of the AI life cycle and the AI supply chain\\nStandards for ethical or responsible AI should be scaled to apply across the AI lifecycle.\\nThe AI life cycle covers design, development, deployment, and operation of AI. Typically, this is an iterative\\nprocess and may involve elements from different kinds of suppliers in an extended supply chain. Different\\nsuppliers may be involved in each of the following AI supply chain processes:\\n\\u2022 Design, implementation, and dispersal of model structure;\\n\\u2022 Collation of training and testing data sets;\\n\\u2022 Actual training on data sets;\\n\\u2022 Provision of pre-trained weights;\\n\\u2022 \\u201cFine Tuning\\u201d where weights are tweaked to fit a new context;\\n\\u2022 Deployment of a model for inference;\\n\\u2022 Provision of the final service to end-users.\\nThere are at least three considerations for good regulatory design that arise from an understanding of the\\nAI supply chain:\\n1. First, the liability of the suppliers in the supply chain, behind the point of supply, is often highly\\nuncertain. For example, where an AI causes harm, the final provider of that system may be liable\\nunder general law or statute. But the responsibility of suppliers, such as the person who provided\\nthe data set, behind that provider may be unclear. The parties in the supply chain can of course\\ndeal with these risks through contract. However, uncertainty and opacity may lead to inefficient\\nand ineffective allocations of risk as between suppliers, while also failing to adequately reduce the\\nrisks to the individuals who are subject to the outputs of the system.\\n2. Secondly, features of the AI lifecycle mean that any mandatory standards should be scaled to the\\noperations and application of the AI tool. This is because the nature and scope of the risks\\nattaching to any one part of the lifecycle will vary. Parties will need a clear account of the nature\\nand scope of risks specific to their stage of the supply chain and the character of their contribution\\nto an AI product in order to know what to look for. This specificity and precision in AI governance\\ncomes from adopting a risk-based approach, which we also support.\\n3. Thirdly, while large models provided by well-resourced actors have been the focus of much\\ndiscussion, we warn that powerful models are available in open-source form. These models are\\npublicly developed and contributed to by decentralised sets of individuals whose identities may\\nbe difficult to discern. Code and model weights are posted on public repositories under\\npseudonyms for other users to download and run on their own hardware. Recent results show\\nthat large language models can be fully hosted on an iPhone and have power comparable (those\\nlesser than) those running on large server farms. This poses a challenge to regulating the\\ntechnology, rather than specific offerings of the technology to consumers.\\nGatekeepers\\nTo some extent the incentive for firms to comply with ex ante risk assessment obligations comes from\\ngeneral law and legislation. Failure adequately to assess and respond to the risks of AI may lead to liability\\nunder these regimes, e.g. as misleading conduct, negligence, or a failure to engage in conduct that is\\nefficient, honest, and fair under financial services regulation. We further suggest reporting obligations\\nmight attach to corporate gatekeepers in the AI lifecycle, namely firms that are supplying AI products or\\nservices (as opposed to elements of those products) to other businesses or consumers.\\n3. Are there any further non-regulatory initiatives the Australian Government could implement to\\nsupport responsible AI practices in Australia? Please describe these and their benefits or impacts.\\n7\\nWe consider that there are at least two non-regulatory initiatives that the Australian Government might\\nimplement to support responsible AI practices in Australia: funding regulatory oversight and best practices\\nstandards for its own use of AI.\\nFunding in house AI expertise within regulators\\nThe Australian Competition and Consumer Commission (ACCC) is an effective and active regulator. We\\ndraw attention to the significant success already shown by the ACCC15 in enforcing the Australian\\nConsumer Law in applying to technology driven services.16\\nHowever, effective regulation increasingly requires technical expertise. Regulators need strong powers\\nto gather information in investigating and enforcing regulatory compliance. Additionally, these powers\\nneed to be accompanied by applied research and data-analysis capabilities\\u2013capabilities that regulators\\nworldwide are still in the process of developing. We recommend that relevant regulators should be\\nfunded to develop and maintain this necessary technical expertise to enforce both general law and AI\\nspecific obligations. Such in house expertise should be complemented by a Standing Expert Advisory\\nGroup, as we in response to question 4 below.\\nOne small-scale model is the Office of Technology Research and Investigation (OTECH) at the U.S. Federal\\nTrade Commission (FTC), created to \\u201clevel[] the playing field and empower[] the FTC to better tackle\\nabuses from technology companies.\\u201d The success of OTECH, despite constrained resources, has motivated\\nlegislation to fund fully staffed Bureau of Technology and the hiring of a Chief Technologist.17 Australia\\nmight learn from this experience, by providing secure funding to develop AI expertise to key regulators.\\nGovernment modelling of best practice ethical AI governance and risk assessment\\nThe government has a valuable opportunity to send a message to organisations that use AI through its\\nown approach to responsible AI practices. Appropriate policy settings for central departments, portfolio\\nagencies and SOEs in this area signal a seriousness of intent, and work to socialise and normalise such\\napproaches in the wider corporate and non-governmental spheres, as well influence policy settings for\\nother levels of government in Australia.\\nSuggested settings include visible compliance with responsibility-promoting policies in the way that public\\nsector organisations treat AI, but also mandating practices at appropriate levels for external organisations,\\nmatched to the risk profile of the particular technology solution in question, during the tendering process\\nand when contracting with suppliers.\\n4. Do you have suggestions on coordination of AI governance across government? Please outline the\\ngoals that any coordination mechanisms could achieve and how they could influence the development\\nand uptake of AI in Australia\\nWe support the government establishing an advisory body to oversee its own use of AI and ADM.18 We\\n15ACCC, \\u2018Trivago Mislead Consumers About Hotel Room Rates\\u2019 (Media Release) https://www.accc.gov.au/media-release/trivago-\\nmisled-consumers-about-hotel-room-rates; ACCC, \\u2018Trivago to Pay 447 Million in Penalties for Misleading Consumers Over Hotel\\nRoom Rates\\u2019 (Media release) https://www.accc.gov.au/media-release/trivago-to-pay-447-million-in-penalties-for-misleading-\\nconsumers-over-hotel-room-rates.\\n16 See eg Liam Harding, Jeannie Marie Paterson, Elise Bant, \\u2018ACCC vs Big Tech: Round 10 and counting\\u2019 Pursuit (24 March 2022).\\n17FTC https://www.ftc.gov/technologists-; :\\nhttps://www.wyden.senate.gov/imo/media/doc/Wyden%20Privacy%20Bill%20Discussion%20Draft%20Nov%201.pdf (see\\ndiscussion to draft bill);Federal Trade Commission, \\u2018Office of Technology Hiring\\u2019 (Webpage) https://www.ftc.gov/technologists;\\nSee also the discussion to a draft bill to fund a fully staffed Bureau of Technology, and hiring of a Chief Technologist:\\nhttps://www.wyden.senate.gov/imo/media/doc/Wyden%20Privacy%20Bill%20Discussion%20Draft%20Nov%201.pdf.\\n18 Recommendation 17.2 of the Report of The Royal Commission into the Robodebt Scheme recommends the \\u2018Establishment of\\na body to monitor and audit automated decision-making\\u2019: Royal Commission into the Robodebt Scheme (Report, 7 July 2023) vol\\n1, xvi.\\n8\\nnote that many of the existing legal regimes that will apply to regulate the use of AI by private sector bodies\\nin making decisions about and providing services to individuals do not apply to government (e.g.\\nCorporations Law; Australian Consumer Law). An AI Advisory Group would additionally provide expert\\ninsight, advice and recommendations to government, parliament, and regulators.\\nThere is currently no representative organisation that coordinates AI expertise across the country. The\\nNational AI Centre at CSIRO might be a convenor but currently has uneven representation (being focused\\non NSW). We suggest that this AI Advisory Group should have membership from a diverse range of\\nstakeholders \\u2013 industry, tech, policy, researchers, and people with lived experience of the outputs of AI\\nand ADM. Members should be drawn from diverse backgrounds and be representative of the whole of\\nAustralia.\\nResponses Suitable for Australia\\n5. Are there any governance measures being taken or considered by other countries (including any not\\ndiscussed in this paper) that are relevant, adaptable, and desirable for Australia?\\nWe suggest that Australia should take guidance from initiatives in other countries, including near\\nneighbours such as Singapore and trading partners such as the UK, US, and Canada. These initiatives are\\nall helpfully detailed in the consultation document.\\nWe strongly recommend that Australia\\u2019s actions in this field are designed with regard to best international\\npractice, and to complement that practice. While regulation for responsible AI is a driving purpose of\\nreform, there is no benefit in reform that increases compliance costs of innovation without proportionate\\nimprovement in outcomes. Australian firms dealing overseas and international firms operating in Australia\\nmay need to comply with multiple regulatory regimes. Ideally Australia's requirements complement and\\nare compatible with those in key overseas markets (without lowering national expectations or standards).\\nTarget Areas\\n6. Should different approaches apply to public and private sector use of AI technologies? If so, how should\\nthe approaches differ?\\nPrivate business and government respond to different pressures and so different types of incentives and\\ndisincentives to identify and respond to potential risks of AI may be appropriate. We also consider that\\ngovernment should hold itself to the highest standards of ethical AI practice. As discussed above in\\nresponse to question 4, governments are in a good position to set exemplary standards, trial and\\ndemonstrate best practices.\\nMore generally, however, we consider that AI should be regulated by reference to its outputs and the\\ncharacter of the service provided e.g. utilities, health services, financial management, and education.\\nPrivate bodies provide essential and necessary services that have profound effects on people\\u2019s lives. The\\nuse of AI in these contexts should be done to high standards of governance, care, and regard to human\\nrights regardless of the identity of the provider. Any design or deployment of AI, but particularly where it\\ntouches on fundamental rights, should proceed with regard to user experience, and human-centred design\\nprinciples.\\n7. How can the Australian Government further support responsible AI practices in its own agencies?\\nWe think there is an imperative for government to develop its own procurement practices that are\\ncompliant with a rigorous risk assessment process and consistent with principles of AI ethics.\\n8. In what circumstances are generic solutions to the risks of AI most valuable? And in what\\ncircumstances are technology-specific solutions better? Please provide some examples.\\n9\\nAs noted above at 2.3, we think effective regulation of AI is best understood as multi-faceted, or as a\\nregulatory network, spanning across the AI lifecycle and supply chain. This means that an effective regime\\nmay include soft law guidelines, AI standards and generally applicable or sector specific law. AI specific\\nregulation might best be used to embed ex-ante risk assessment processes and proportionate responses\\nto this risk assessment in terms of  transparency, explanations, and accountability mechanisms. AI specific\\nlaw may also be required for uses of technology judged high risk and warranting additional regulation.\\nPrinciples based law, found in regimes such as the Australian Consumer Law, are appropriate as a safety\\nnet to respond to those harms that arise despite a risk assessment process, and ex-ante interventions.\\nGeneral law and legislation also provide an incentive to take seriously the need to include safety, privacy,\\naccountability etc \\u2018by design\\u2019.19 Sector specific law, such as financial services have a role in governing AI,\\nlike other interventions, according to the norms and standards imposed on all participants in that market.\\n 9. Given the importance of  transparency across the AI lifecycle, please share your thoughts on:\\na. Where and when  transparency will be most critical and valuable to  mitigate potential AI risks\\nand to improve  public trust and confidence in AI?\\nWe think  transparency, along with explainability, is important but not the entire solution. See also our\\ndiscussion above at 2.4.\\n Transparency\\n Transparency can be important in promoting safe and responsible AI. But care needs to be taken not to\\noverload the concept of  transparency in a way that undermines its utility.\\nThe concept of  transparency in AI regulation should be used precisely and not muddle the varying uses\\nfor  transparency.  Transparency may be used to provide visibility for:\\n\\u2022 governance (auditing/compliance),\\n\\u2022 internal operations (ensure companies are encouraged and able to notice the right sorts of things),\\n\\u2022 regulatory oversight (allowing regulators to verify claims made by firms about their AI tools and\\nprocesses, as well as enforce relevant law);\\n\\u2022 advocates and lawyers in pursuing compensation and redress for AI harms; and\\n\\u2022 journalists and public interest organisations in ensuing those who deploy AI face meaningful public\\naccountability;\\n\\u2022 consumers (to respect autonomy and empower decision-making or choice).\\nThe demands of \\u2018 transparency in AI will differ between these various uses.\\nImportantly,  transparency should not merely mean providing users/subjects of AI with \\u201cmore info\\u201d\\n(especially with huge datasets and complex code). Nor should  transparency be used as a reason by firms\\nfor themselves failing to take adequate steps for responsible AI. Merely telling individuals about the AI or\\nhow it \\u2018works\\u2019 should not make users/subjects solely responsible for protecting themselves against\\npossible risks arising from that use of AI.\\nA further generic solution is to require AI system builders to provide baseline information about their use\\nof AI. Typically, this means providing the details of:\\no the source data used to train machine learning algorithms: who collected the data, what\\nmeasures of bias were checked and performance against these metrics, how\\nrepresentative training data is of the actual source materials how current is the training\\ndata, and other similar necessities;\\n19Jeannie Marie Paterson, \\u2018Making Robo-advisers careful? Duties of care in providing automated financial advice to consumers\\u2019\\n(2023) 18 Law and Financial Markets Review.\\n10\\no the training regime used: which data preparation steps were performed, which machine\\nlearning algorithm(s) were applied, what parameters were used to guide them, what the\\nparameter values were, what the learning outputs were;\\no the achieved performance results of training: what the metrics of evaluation were, what\\nthe systems\\u2019 output scores were, what gold standard material was used for comparison,\\nwho prepared the gold standard material, and how was it prepared\\no the use of the trained system in the current application: how the system was adapted to\\nthe application, the parameters of run-time application, what was the impact of\\nadaptation on performance, how well the system performs in the current application,\\nwhat the gold standard is for measurement of current performance, how this gold\\nstandard material was prepared or obtained;\\no any further observations and/or measurements on the bias, gaps, and other performance\\nstatistics of the system applied to the current application;\\nExplanations\\nThe need for AI systems (particularly LLM-based generative AI systems) to explain their reasoning has been recognised for well over a decade.20 But there are still no general and standard practices or methodologies that specify what an adequate explanation is exactly and how to produce one, although the issue is the topic of considerable research.21 Existing explanation strategies (including hotspot analysis, meta- networks that interpret others, explanatory diagnosis using test cases, etc.) exist and should be encouraged where appropriate. Like  transparency, explanations are not a panacea for responsible AI, and have limitations such as in identifying unlawful or unfair bias.22\\n Transparency and Privacy\\nThe requirements of data protection and privacy can conflict with the above suggestions for  transparency and explanations, since these features will often require access to personal data, and indeed may risk disclosing such information.\\nMachine learning models are trained on data. This data, depending on the setting, comes either from scraping the Internet, company\\u2019s users (e.g., emails of customers used to train auto-complete features during email composition) or other proprietary sources. Privacy research has demonstrated the reverse- engineering of this training data from access to machine learning model APIs (e.g., by giving prompts to a language model or image generation service). To this end, one has to carefully consider and understand where training data came from and if it has privacy and/or intellectual property rights attached to it, since these rights can be blatantly violated if used in ML model.\\nThe California Consumer Privacy Act of 201823 encodes a right to erasure similar to the GDPR, requiring that organisations remove data without unreasonable delay when requested. Recently the U.S. Federal\\nTrade Commission found that this extends to derived products of data, including AI models trained on data.24 The area of \\u201cmachine unlearning\\u201d has emerged as a technical mitigation to assist organisations\\n20 Wikipedia \\u2018Explainable Artificial Intelligence\\u2019 (Webpage) .\\n21See eg Tim Miller, \\u2018Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven Decision Support using Evaluative AI\\u2019\\n(Conference Paper, FAccT \\u201923: Proceedings of the 2023 ACM Conference on Fairness, Accountability and  Transparency, June\\n2023); Mor Vered, Tali Livni, Piers Douglas Lionel Howe, Tim Miller and Liz Sonenberg, \\u2018The Effects of Explanations on\\nAutomation Bias\\u2019 (2023) 322(9) Artificial Intelligence.\\n22Jeannie Paterson, \\u2018Misleading AI: Regulatory Strategies for Algorithmic  Transparency in Technologies Augmenting Consumer\\nDecision-Making\\u2019 (2023) 34 Loyola Consumer Law Review 558.\\n23 California Consumer Privacy Act 2018 1.81.5 Cal Civ Code. 5\\n24\\u2018California Company Settles FTC Allegations It Deceived Consumers about use of Facial Recognition in Photo Storage App\\u2019 FTC\\n(Media Release) < https://www.ftc.gov/news-events/news/press-releases/2021/01/california-company-settles-ftc-allegations-it-\\n11\\nwith efficient removal of data from downstream models. 25 We note however that machine unlearning is\\nstill in its infancy and may raise privacy concerns of its own while remaining a valuable area for future\\nexploration.\\nb.  Mandating transparency requirements across the public and private sectors, including how\\nthese requirements could be implemented\\nAs discussed above at 2.4, we think that the use of mandatory standards should be proportionate to risk\\nwith greater expectations placed on gatekeeper firms that supply AI products or utilise AI decision-making\\nthat affects individuals or small businesses.\\nModel cards for AI provided as a service to consumers and (small) business\\nAdditionally,  transparency requirements may be useful in the context of individuals and businesses buying\\nAI services (as opposed to being subject to AI outputs, such as through ADM). In engaging with AI products,\\nat this point in time, consumers and (smaller) businesses both lack adequate information to determine if\\na given use of AI is appropriate in a given context.\\nOne possibility is the use of standard disclosures or \\u2018model cards\\u2019 specifying various element of the AI\\nbeing offered. Indeed, we suggest that Governments might promote the use of model cards by making\\nthis a condition of procurement.\\nModel cards are short displays of information provided by developers on release of a model. Model cards\\nmay provide information as to the provenance of training data, known failure modes, and basic use\\ninformation. However, this practice is currently entirely voluntary and lacks consistency with respect to\\nthe depth, quantity, and quality of information provided.\\nSeparate model cards should be provided for B2B and B2C offerings respectively. Consumers are not well\\nplaced or incentivized to evaluate the many tools they will likely encounter; however clear guidance may\\nimprove consumer ability to adopt AI in contexts where it makes the most sense. Model cards that describe\\nintended uses, failure modes, and data provenance should be provided alongside consumer offerings of\\nmodels that meet certain thresholds.26\\n10. Do you have suggestions for Bans?\\nWe consider that bans should be targeted at uses of AI, not specific technologies. We think there are some\\nuses of AI that should be banned. However, we consider bans should be imposed on the basis of clear\\ncompelling criteria and attenuated to context. This approach protects innovation and makes the\\nimposition of a ban more compelling and likely to withstand the test of time. This approach is also\\nconsistent with the principles of good regulatory design discussed above. Thought might also be given to\\nsunset provisions on bans (i.e. temporal limit).\\na. Whether any high-risk AI applications or technologies should be banned completely?\\nSubject to the above qualifications we consider bans should be considered for uses of AI with high risk to\\nhuman rights. In particular, we single out most forms of biometric surveillance in public spaces (eg FRT, iris\\nor gait recognition, emotion detection) and further consider these technologies should only ever be used\\n(if at all) in other contexts a high level of regulatory oversight and control.\\nWe do not consider that bans should be imposed on particular algorithms, software or even hardware. We\\ndeceived-consumers-about-use-facial-recognition-photo>.\\n25Chuan Guo et al, \\u2018Certified Data Removal From Machine Learning Models\\u2019 (2020) 119 Proceedings of the 37th International\\nConference on Machine Learning 3832.\\n26 We defer discussion as to which models should be regulated to other parts of this submission.\\n12\\nthink the potential harms from AI are real. But bans on elements as opposed to uses is futile. For example,\\nbans on high performance compute (i.e. GPUs/TPUs) has sometimes been discussed. Such bans are easily\\nevaded by using different hardware combinations or remote computing to reach the equivalent\\nperformance.\\nb. Criteria or requirements to identify AI applications or technologies that should be banned, and\\nin which contexts?\\nRelevant considerations for bans might include:\\n\\u2022 applications where input into AI/ML models can be easily manipulated (see Adversarial\\nManipulations above 2.4)\\n\\u2022 human rights impacts;\\n\\u2022 public policy considerations;\\n\\u2022 circumstances where can\\u2019t rely on consent to justify use or consent is not a valid or\\nproportionate way of justifying use given the impact on fundamental rights;\\n\\u2022 concentrations of private power or resources.\\n\\u2022\\n11. What initiatives or government action can increase public trust in AI deployment to encourage\\nmost people to use AI?\\nWe consider that regulatory regimes requiring good AI governance, including through risks assessments\\nand other accountability measures will increase the trustworthiness of AI.\\nWe also note that there is considerable uncertainty around AI within business and society generally.\\nTherefore, education and training, as we noted in the overview of this submission, to demystify AI, are\\ncrucial.\\nSome degree of clarification about existing law, and even in some contexts the use of safe harbours and\\nsandboxes may prove beneficial for business seeking to innovate with AI, provided these are matched with\\nstrong baseline standards for safety and ethical/responsible practice.\\nWe note that industry uptake of the internet was substantially encouraged by a variety of laws, among\\nthem Title II of the U.S. Digital Millennium Copyright Act (DMCA) of 1998. This title created a safe harbour\\nfrom copyright liability for online service providers from user uploaded content\\u2014so long as platforms\\nundertook certain requirements to act responsibly. Absent this section, it is likely that substantial portions\\nof the internet\\u2019s innovations never would have happened. AI finds itself in a similar position.\\nImplications and Infrastructure\\n12. How would banning high-risk activities (like social scoring or facial recognition technology in\\ncertain circumstances) impact Australia\\u2019s tech sector and our trade and exports with other countries?\\nWe consider there is no competitive advantage in promoting harmful technologies. Australia\\u2019s technology\\nsector is capable and indeed is developing cutting edge technologies that have beneficial impacts, such as\\nfor example in the med tech and ag tech fields.\\n13. What changes (if any) to Australian conformity infrastructure might be required to support\\nassurance processes to mitigate against potential AI risks?\\nWe do not have any comments on this question.\\nRisk-based approaches\\n13\\n14. Do you support a risk-based approach for addressing potential AI risks? If not, is there a better\\napproach?\\nWe support a risk-based approach to address potential AI risks, however, as discussed above in response\\nto question 2, we think careful consideration needs to be given to an adaptive and flexible approach that\\nis compatible with Australia\\u2019s existing regulatory regime. The EU AI Act uses a risk-based model. A key\\ncritique of the EU AI act is that it is overly prescriptive without being backed by strong oversight and\\nenforcement mechanisms.\\nRisk based governance for AI should require firms to make a judgement about the risk threat arising from\\nthe outputs of their products and implement appropriate governance strategies in response, including as\\nto  transparency and accountability. Models for these approaches are already in existence e.g. NIST27 or in\\nSingapore.28 The key is to identify mechanisms for requiring these assessments to take place (as a legal\\nobligation) and to ensure they are robust and effective (reporting / monitoring).\\n15. What do you see as the main benefits or limitations of a risk-based approach? How can any\\nlimitations be overcome?\\nRisk based approaches have the attraction of being able to be integrated into existing risk management\\nand auditing regimes within corporations, both internal and statutory. Such approaches are not perfect,\\nbut would at least it force a practice of identification, quantification, mitigation, and someone having to\\nsign-off on what residual risks remain.\\nHowever, we argue that a risk-based approach that relies on predetermined categories of risk, as opposed\\nto a responsive model scaled to use and context as we have advocated in response to question 14, has a\\nnumber of inherent caveats which require consideration.\\nFirstly, a key concern with AI surrounds the unpredictability of outcomes. An approach which relies solely\\non our ability to anticipate risks is unlikely to account for all possible eventualities.\\nSecondly, determining the threat level will evolve over time, as the technology develops. Maintaining a\\ncategorisation that accounts for changing technologies would be challenging, if not infeasible.\\nWe recommend a more holistic approach that takes account of the context of the use and the profile of\\nthe entity utilising the AI. Assessment should be dynamic and ongoing.\\n16. Is a risk-based approach better suited to some sectors, AI applications or organisations than\\nothers based on organisation size, AI maturity and resources?\\nAll organisations using AI should be using a risk approach. However, we consider that approach should\\nbe scaled to the risk presented (rather than the size of the organisation). See also above comments on\\nmodel cards in response to question 9(b).\\n17. What elements should be in a risk-based approach for addressing potential AI risks? Do you\\nsupport the elements presented in Attachment C?\\nWe support the elements presented in attachment C, but would go further in what may be required from\\norganisations.\\n27 \\u2018AI Risk Management Framework\\u2019 NIST (Webpage) < https://www.nist.gov/itl/ai-risk-management-framework>.\\n28\\u2018Companion to the Model AI Governance Framework\\u2019 World Economic Forum, prepared in collaboration with Info-\\ncommunications Media Development Authority of Singapore (Report, January 2020).\\n14\\n\\u2022 We suggest a more nuanced understanding of the different roles that may be played by concepts\\nof  transparency and explanations, see comments in response to question  transparency/explanations are important, we also emphasise the importance\\nof a focus on monitoring/auditing outputs. This will often for example be a more effective way of\\ntesting for bias, drift etc.\\n\\u2022 We note the advantages of making risk assessments and other interventions consistent with\\ninternational initiatives as far as possible and prudent. (For example we point to the NIST initiatives\\nin the US).\\n\\u2022\\n18. How can an AI risk-based approach be incorporated into existing assessment frameworks (like\\nprivacy) or risk management processes to streamline and reduce potential duplication?\\nThis can be done by allowing flexibility and a risk response that is tailored to the application and its likely\\nimpact.\\n1 Transparency and Decision-Making Accountability: Thoughts for buying machine learning\\nalgorithms' in Office of the Victorian Information Commissioner (ed), Closer to the Machine: Technical, Social, and Legal aspects\\nof AI (2019)\\n30Mor Vered, Tali Livni, Piers Douglas Lionel Howe, Tim Miller and Liz Sonenberg, \\u2018The Effects of Explanations on Automation\\nBias\\u2019 (2023) 322(9) Artificial Intelligence.\\n15\\nAnd should it apply to:\\na. Public or private organisations or both?\\nSee above response to q6.\\nb. Developers or deployers or both?\\nSee above 2.4. We consider all initiatives for responsible AI, including risk-based approaches, should apply to across the supply chain but with more demanding obligations on key gatekeepers.\\n16\",\n          \"transparency, oversight, and corrective action requirements, or it failed to ensure adequate accuracy, robustness and cybersecurity of the system.42\\n34\\nIbid at [152] (Moshinsky and Derrington JJ).\\n35\\nIbid at [75] (Kerr J, in dissent).\\n36\\nEuropean Union, Treaty on the Functioning of the European Union, 26 October 2012, OJ L. 326/47-326/390.\\n37\\nLiability Directive (n 2) Preamble.\\n38\\nIbid, art 2(1).\\n39\\nIbid, art 3(1).\\n40\\nIbid, art 3(5).\\n41\\nIbid, art 4(1).\\n42\\nIbid, art 4(2).\\nThe Department should consider an Australian law \\u2013 quite likely by subordinate legislation to enable flexibility and speed in making regulatory changes \\u2013 which enacts provisions similar to the EU\\u2019s AI\\nLiability Directive. Such a Directive should be clearly pointed to providing the judiciary with boundaries on determining causation and the existence / breach of duty in cases of negligence arising from the use of AI systems.\\nCoordination of AI Governance (Question 4)\\nGiven the issues identified above, there should be a clear consideration of the Recommendations 17.1 and 17.2 from the Robodebt Royal Commission, which stated:\\nRecommendation 17.1: Reform of legislation and implementation of regulation\\nThe Commonwealth should consider legislative reform to introduce a consistent legal framework in which\\nautomation in government services can operate.\\nWhere automated decision-making is implemented:\\n\\u2022 there should be a clear path for those affected by decisions to seek review\\n\\u2022 departmental websites should contain information advising that automated decision-making is used\\nand explaining in plain language how the process works\\n\\u2022 business rules and algorithms should be made available, to enable independent expert scrutiny.\\nRecommendation 17.2: Establishment of a body to monitor and audit automated decision-making\\nThe Commonwealth should consider establishing a body, or expanding an existing body, with the power\\nto monitor and audit automate decision-making processes with regard to their technical aspects and their\\nimpact in respect of fairness, the avoiding of bias, and client usability.\\nThe Government should consider creating a body (or more likely, expanding one) in the terms recommended in the Robodebt report to appropriately establish, coordinate and monitor the use of\\nAI in circumstances where \\u201cdecisions\\u201d are being made, either by Government or actors empowered by government (i.e., contractors or service providers).\\nResponses suitable for Australia (Question 5)\\nOne option that should be strongly considered is a discretionary power vested in the Minister for\\nIndustry, Science and Resources (or similar portfolio) to issue a determination that a particular system or class of systems should be regarded as \\u201cAI\\u201d for the purposes of Australia\\u2019s legislation. Rather than needing statutory change to alter the definition of AI, the Minister should have the power \\u2013 after consulting with the public \\u2013 to include a particular system or class of system within Australia\\u2019s regulatory framework. A similar \\u201clicensing\\u201d system for AI has been proposed (but not yet adopted) in the United Kingdom.43\\nThe proposed scheme could operate in the same manner as the Security of Critical Infrastructure Act\\n2018 (Cth) (SOCI Act). Under the SOCI Act, the Home Affairs Minister may44 \\u2013 after consulting with the asset45 and in some cases the First Ministers of the States and Territories46 (as well as any other\\n43\\nKiran Stacy, \\u2018AI should be licensed like medicines or nuclear power, Labour suggests\\u2019, The Guardian (online, 6\\nJune 2023) .\\n44\\nSecurity of Critical Infrastructure Act 2018 (Cth), s 51(1).\\n45\\nIbid, ss 9(1)(f) and 9(3).\\n46\\nIbid, s 9(4).\\npersons the Minister considers necessary47) \\u2013 the Minister may prescribe an asset to be a \\u2018critical infrastructure asset\\u2019 or \\u2018system of national significance\\u2019.48 Upon prescription, the asset then becomes subject to various security obligations in the SOCI Act.\\nIn a similar fashion, the EU Act delegates authority to the European Commission to add to the uses listed in the annex where a new use is envisaged, falls within one of the above categories, and \\u2018poses a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights\\u201949 that is equivalent or greater than the risk posed by one of the uses already included in the annex. This is in addition to the prescribed eight categories of \\u201chigh risk AI system\\u201d which are:\\n\\u2022 Biometric identification and categorisation;\\n\\u2022 Operation of critical infrastructure - i.e., safety components for road traffic management and\\nthe supply of water, gas, heating and electricity;\\n\\u2022 Education and vocational training - including entry exams, student assessments;\\n\\u2022 Employment - recruitment, promotions, performance evaluations, and task allocation;\\n\\u2022 Essential private services and public services and benefits such as welfare, credit assessments\\nand operations of emergency services;\\n\\u2022 Law enforcement and immigration control including individual risk assessments, profiling\\nindividuals, crime analytics and predicting offending; and\\n\\u2022 Judicial administration, including systems intended to assist a judicial authority in factual and\\nlegal research.50\\nAnother measure would be the imposition of \\u201cred lines\\u201d \\u2013 areas of AI development which are considered too high risk to be developed in Australia. Under the EU\\u2019s AI Act for example, certain uses of AI are deemed to be \\u2018unacceptable\\u201951 and are therefore prohibited. These uses are, broadly, systems that create material \\u2018behavioural distortion,\\u201952 social credit systems53 like those already deployed in parts of China,54 and real-time remote biometric surveillance for law enforcement purposes,55 the latter prohibition being subject to a number of qualifications and exceptions.56\\nRisk-Based approaches to AI Regulation (Questions 14 and 15)\\nThe Discussion Paper refers to the UTS Model Facial Recognition Law as one example of a risk-based regulatory scheme which could apply to AI. The EU AI Act (in Attachment B of the Discussion Paper) is another. The inherent principles in Canada\\u2019s Bill C-27 are yet another.\\nHowever, each of these applications presupposes several questions which cannot be answered in the abstract. If risk is \\u2018the effect of uncertainty on the achievement of objectives\\u2019,57 then it must be asked who does the risk affect? Whose objectives take priority in the assessment of risk? Who determines\\n47\\nIbid, s 9(6).\\n48\\nIbid, ss 51A(1) and 52B(1) respectively.\\n49\\nAI Act art 7.\\n50\\nNoting the breadth of the definition of AI - this could extend to using web search.\\n51\\nAI Act art 5(1).\\n52\\nAI Act arts 5(1)(a) and (b).\\n53\\nAI Act art 5(1)(c).\\n54\\nFor a discussion of these systems in the context of China see M. Zalnieriute, L. Bennett Moses and G.\\nWilliams, \\u2018Automating Government Decision Making: Implications for the Rule of Law,\\u2019 in S. de Souza, M. Spohr\\n(eds), Technology, Innovation and Access to Justice: Dialogues on the Future of Law, Edinburgh University\\nPress, UK, 2021, at 91.\\n55\\nAI Act art 5(1)(d).\\n56\\nAI Act Arts 5(2) and (3).\\n57\\nInternational Standards Organization, ISO 31000: Risk management (2018).\\nthe risk appetite, i.e., what is an \\u201cacceptable\\u201d level of risk? And who owns the residual risk related to a given AI capability or program? The Discussion Paper does not confront those questions. Although a risk-based approach to AI regulation may be appropriate, it cannot be considered without determining the answer to each question, which raises a number of global issues.\\nThe first issue relates again to the notion of regulatory arbitrage. Under the EU AI Act, there are certain technologies and capabilities which are simply \\u201cbanned\\u201d, i.e., those which have a \\u2018significant potential to manipulate persons through subliminal techniques, exploit the vulnerabilities of specific vulnerable groups and AI-based social scoring done by public and private authorities\\u2019.58 Yet who is responsible for determining compliance with this requirement? If the onus falls on the companies developing these technologies, they may be able to rely on semantics to defend their position, i.e.:\\n\\u2022 The technology does not have a significant potential to manipulate persons (but the potential\\nis still sufficient to generate concern);\\n\\u2022 The technology does not exploit the vulnerability of a specific group (but perhaps multiples\\ngroups or the entirety of the public at large);\\n\\u2022 The technology does not produce a social score (but does permit other forms of classification\\nor ranking).\\nSecondly, the balance between individual rights and the \\u201cpublic interest\\u201d does not always resolve favourably in Australia (as we lack a constitutive Charter or Act on human rights). In recent cases of technology overreach by government \\u2013 the use of Auror and ClearView AI by the Australian Federal\\nPolice, or the cases of the Robodebt or Dutch tax scandals \\u2013 these actions have been justified by appeals to public interests such as detection of crime and prevention of fraud. Yet very clearly, there have been breaches of human dignity and privacy, and which often are the first victims in cases of\\nState surveillance.59\\nThirdly, companies operating in risk-based environments often have a vested interest in the rapid deployment of technologies (which will include AI technologies), even where they carry a potential compliance liability. The launch of Uber as a ride-sharing service in Australia in 2012 was known to be an illegal operation, running without permits required by State and Territory governments.60 Uber also operated an algorithm known as \\u201cGreyball\\u201d, helping them spot traffic inspectors trying to \\u201csting\\u201d unlicensed drivers.61 Companies are already racing ahead to develop and launch generative AI content without properly assessing the risks of those same technologies. 62\\nFourthly, in many instances of current technologies, government already carries a significant compliance burden which AI should not be seen as adding to. For example, under the Privacy Act there is no independent remedy for affected individuals for breach of privacy (such as a tort or private cause of action). This leaves the government, and in particular the Office of the Australian Information\\nCommissioner, as the sole investigative and compliance body for ensuring companies do the right\\n58\\nAttachment B to the Discussion Paper.\\n59\\nAlso see for example Brendan Walker-Munro, \\u2018Hyper-Collection: A Possible New Paradigm in Modern\\nSurveillance\\u2019 (2023) 21(2) Surveillance & Society 120-138.\\n60\\nBen Butler, \\u2018The Uber files: firm knew it launched illegally in Australia, then leaned on governments to change the law\\u2019, The Guardian (online, 15 July 2022) https://www.theguardian.com/news/2022/jul/15/the- uber-files-australia-launched-operated-illegally-document-leak.\\n61\\nJulia Carrie Wong, \\u2018Greyball: how Uber used secret software to dodge the law\\u201d, The Guardian (online, 4\\nMarch 2017) https://www.theguardian.com/technology/2017/mar/03/uber-secret-program-greyball- resignation-ed-baker.\\n62\\nNatasha Lomas, \\u2018Don\\u2019t rush generative AI apps to market without tackling privacy risks, warns UK watchdog\\u2019,\\nTechCrunch (15 June 2023) https://techcrunch.com/2023/06/15/uk-ico-generative-ai-warning/.\\nthing. Nor should the responsibilities be solely left to the individual, acting as either citizen or customer, to have to determine whether their interactions with AI fall at the high-risk or low-risk end of the spectrum.\\nConclusion\\nThe policy and legislation settings of Australia will require significant consultation and discussion to properly enact a framework which respects both the technological but people-centred use of AI in the future. This submission has made only brief proposals in relation to specific areas of interest.\\nTherefore, we would be happy to provide further details, or attend further consultation, as determined by the Department on any other issues that may arise during this process.\\nThank you for the opportunity to make this submission.\\nProf Rain Liivoja, Deputy Dean (Research) and Research Lead, Law and the Future of War Research\\nGroup, The University of Queensland\\nEmail r.liivoja@uq.edu.au\\nDr Brendan Walker-Munro, Senior Research Fellow, The University of Queensland\\nEmail b.walkermunro@uq.edu.au\\nDr Lauren Sanders, Senior Research Fellow, The University of Queensland\\nEmail l.sanders@uq.edu.au\\nDr Sam Hartridge, Senior Research Fellow, The University of Queensland\\nEmail s.hartridge@uq.edu.au\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export df_files to an Excel file\n",
        "if df_files is not None:\n",
        "    excel_filepath = \"/content/drive/MyDrive/submission_content_transparency.xlsx\"  # Replace with desired path\n",
        "    df_files.to_excel(excel_filepath, index=False)  # Set index=False to avoid writing row indices\n",
        "    print(f\"DataFrame exported to: {excel_filepath}\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_files' is empty or None. Cannot export.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lljGX_BzY_1Z",
        "outputId": "3f31808b-29df-4da1-dc6f-ae84da30e1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame exported to: /content/drive/MyDrive/submission_content_transparency.xlsx\n"
          ]
        }
      ]
    }
  ]
}